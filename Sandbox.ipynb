{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scripts.proj1_helpers as helper\n",
    "import implementations as imp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "boson is -1\n",
    "\n",
    "not boson is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 30)\n",
      "(568238, 30)\n"
     ]
    }
   ],
   "source": [
    "y_train, x_train, ids_train = helper.load_csv_data('train.csv')\n",
    "y_test, x_test, ids_test = helper.load_csv_data('test.csv')\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of boson: 164333\n",
      "Number of other: 85667\n"
     ]
    }
   ],
   "source": [
    "print('Number of boson:', np.count_nonzero(y_train-1))\n",
    "print('Number of other:', np.count_nonzero(y_train+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def col_standardize(x):\n",
    "    means = [np.mean(col) for col in x.T]\n",
    "    stds = [np.std(col) for col in x.T]\n",
    "    return (x - means) / stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def standardize(x):\n",
    "    return (x - np.mean(x, axis=0)) / np.std(x, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "std_x_train = standardize(x_train)\n",
    "std_x_test = standardize(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col_std_x_train = col_standardize(x_train)\n",
    "col_std_x_test = col_standardize(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train[y_train < 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=1, the loss=103.9720770839918\n",
      "Current iteration=2, the loss=98.29625478320199\n",
      "Current iteration=3, the loss=97.31771300537916\n",
      "Current iteration=4, the loss=88.40615271800243\n",
      "Current iteration=5, the loss=101.50329841169668\n",
      "Current iteration=6, the loss=89.44701549601467\n",
      "Current iteration=7, the loss=97.35057941519068\n",
      "Current iteration=8, the loss=95.31007013952737\n",
      "Current iteration=9, the loss=88.21783028234296\n",
      "Current iteration=10, the loss=92.46359429542717\n",
      "Current iteration=11, the loss=98.07013206913088\n",
      "Current iteration=12, the loss=90.12062195126488\n",
      "Current iteration=13, the loss=94.3216217123457\n",
      "Current iteration=14, the loss=91.02698343194518\n",
      "Current iteration=15, the loss=91.07405057064125\n",
      "Current iteration=16, the loss=91.3213211894993\n",
      "Current iteration=17, the loss=86.11160998669253\n",
      "Current iteration=18, the loss=90.13806569567646\n",
      "Current iteration=19, the loss=85.55535934902909\n",
      "Current iteration=20, the loss=85.56915112322616\n",
      "Current iteration=21, the loss=82.30075844598231\n",
      "Current iteration=22, the loss=88.39456889647329\n",
      "Current iteration=23, the loss=81.78909473103829\n",
      "Current iteration=24, the loss=84.8826413451174\n",
      "Current iteration=25, the loss=81.05930635837964\n",
      "Current iteration=26, the loss=79.94519497509057\n",
      "Current iteration=27, the loss=83.04044838928473\n",
      "Current iteration=28, the loss=80.23356328133102\n",
      "Current iteration=29, the loss=85.66183844281426\n",
      "Current iteration=30, the loss=78.44540162598449\n",
      "Current iteration=31, the loss=80.16490361663\n",
      "Current iteration=32, the loss=79.02008303907796\n",
      "Current iteration=33, the loss=89.70396182044101\n",
      "Current iteration=34, the loss=82.45525938685643\n",
      "Current iteration=35, the loss=88.06816685640396\n",
      "Current iteration=36, the loss=86.05968731920781\n",
      "Current iteration=37, the loss=77.51742379913026\n",
      "Current iteration=38, the loss=82.36150653489176\n",
      "Current iteration=39, the loss=78.35192911520093\n",
      "Current iteration=40, the loss=76.70703625093213\n",
      "Current iteration=41, the loss=79.25363162936421\n",
      "Current iteration=42, the loss=84.3937665442333\n",
      "Current iteration=43, the loss=81.21149927609235\n",
      "Current iteration=44, the loss=79.4420146974578\n",
      "Current iteration=45, the loss=69.85763682604738\n",
      "Current iteration=46, the loss=74.66085634740148\n",
      "Current iteration=47, the loss=77.46612783118164\n",
      "Current iteration=48, the loss=72.20246959861686\n",
      "Current iteration=49, the loss=76.31495305631242\n",
      "Current iteration=50, the loss=81.23579951689015\n",
      "Current iteration=51, the loss=81.4602008734952\n",
      "Current iteration=52, the loss=76.2948764239365\n",
      "Current iteration=53, the loss=85.72774676689696\n",
      "Current iteration=54, the loss=78.42696262857841\n",
      "Current iteration=55, the loss=81.37306930667617\n",
      "Current iteration=56, the loss=91.03458958154997\n",
      "Current iteration=57, the loss=75.82165418232924\n",
      "Current iteration=58, the loss=78.07926690793903\n",
      "Current iteration=59, the loss=78.58265873405108\n",
      "Current iteration=60, the loss=80.78635162297817\n",
      "Current iteration=61, the loss=84.76366398855052\n",
      "Current iteration=62, the loss=76.21931457585262\n",
      "Current iteration=63, the loss=92.36725562219374\n",
      "Current iteration=64, the loss=79.37191001650723\n",
      "Current iteration=65, the loss=80.00584541328597\n",
      "Current iteration=66, the loss=87.04082307041176\n",
      "Current iteration=67, the loss=79.27177310551474\n",
      "Current iteration=68, the loss=85.26904521267011\n",
      "Current iteration=69, the loss=89.62900655465519\n",
      "Current iteration=70, the loss=85.86391326513075\n",
      "Current iteration=71, the loss=78.74575952702206\n",
      "Current iteration=72, the loss=84.67848948582545\n",
      "Current iteration=73, the loss=86.85394134707387\n",
      "Current iteration=74, the loss=79.24871318268481\n",
      "Current iteration=75, the loss=77.58033443972695\n",
      "Current iteration=76, the loss=71.80888213296224\n",
      "Current iteration=77, the loss=83.90282001422489\n",
      "Current iteration=78, the loss=78.20380725318785\n",
      "Current iteration=79, the loss=79.82131262574194\n",
      "Current iteration=80, the loss=70.50707586556848\n",
      "Current iteration=81, the loss=72.72051670816896\n",
      "Current iteration=82, the loss=76.9061695593285\n",
      "Current iteration=83, the loss=82.84287860047132\n",
      "Current iteration=84, the loss=78.94769028506823\n",
      "Current iteration=85, the loss=72.93919702191289\n",
      "Current iteration=86, the loss=82.3278846884527\n",
      "Current iteration=87, the loss=76.16758751014886\n",
      "Current iteration=88, the loss=73.6136465026009\n",
      "Current iteration=89, the loss=81.75501729441464\n",
      "Current iteration=90, the loss=77.16928530393682\n",
      "Current iteration=91, the loss=83.0347006069568\n",
      "Current iteration=92, the loss=78.95723429895293\n",
      "Current iteration=93, the loss=80.3683389716778\n",
      "Current iteration=94, the loss=87.46041923994616\n",
      "Current iteration=95, the loss=87.49253731274399\n",
      "Current iteration=96, the loss=83.27544189870395\n",
      "Current iteration=97, the loss=85.69058173937296\n",
      "Current iteration=98, the loss=79.97383281786503\n",
      "Current iteration=99, the loss=78.43424010622658\n",
      "Current iteration=100, the loss=78.5921413943118\n",
      "Current iteration=101, the loss=66.51220451595601\n",
      "Current iteration=102, the loss=83.76539009328954\n",
      "Current iteration=103, the loss=84.36359039407623\n",
      "Current iteration=104, the loss=86.97811290383208\n",
      "Current iteration=105, the loss=82.97618756800414\n",
      "Current iteration=106, the loss=82.26446451643537\n",
      "Current iteration=107, the loss=91.33308914748119\n",
      "Current iteration=108, the loss=76.267628560366\n",
      "Current iteration=109, the loss=74.69458752845264\n",
      "Current iteration=110, the loss=69.3890680685947\n",
      "Current iteration=111, the loss=80.79782842765826\n",
      "Current iteration=112, the loss=75.86744776696943\n",
      "Current iteration=113, the loss=78.63258964958995\n",
      "Current iteration=114, the loss=75.99601654235991\n",
      "Current iteration=115, the loss=77.74769407280593\n",
      "Current iteration=116, the loss=79.04561567068512\n",
      "Current iteration=117, the loss=67.0721349282363\n",
      "Current iteration=118, the loss=86.23877723254802\n",
      "Current iteration=119, the loss=81.63863123723442\n",
      "Current iteration=120, the loss=76.63484863082158\n",
      "Current iteration=121, the loss=80.86590578342376\n",
      "Current iteration=122, the loss=80.31427825523393\n",
      "Current iteration=123, the loss=73.91345570034\n",
      "Current iteration=124, the loss=81.82354522738234\n",
      "Current iteration=125, the loss=71.53673092258524\n",
      "Current iteration=126, the loss=84.70162808892042\n",
      "Current iteration=127, the loss=77.80972415872708\n",
      "Current iteration=128, the loss=82.58418165427241\n",
      "Current iteration=129, the loss=78.75084956038404\n",
      "Current iteration=130, the loss=74.73954252497435\n",
      "Current iteration=131, the loss=80.17642392092264\n",
      "Current iteration=132, the loss=89.64927166418464\n",
      "Current iteration=133, the loss=80.4711774348115\n",
      "Current iteration=134, the loss=75.11883639306602\n",
      "Current iteration=135, the loss=82.17034978423715\n",
      "Current iteration=136, the loss=78.03587309516072\n",
      "Current iteration=137, the loss=71.19479036076271\n",
      "Current iteration=138, the loss=82.8874407918613\n",
      "Current iteration=139, the loss=75.80098533714045\n",
      "Current iteration=140, the loss=84.29382497239274\n",
      "Current iteration=141, the loss=83.14054808796341\n",
      "Current iteration=142, the loss=76.66294111622477\n",
      "Current iteration=143, the loss=69.47752536312444\n",
      "Current iteration=144, the loss=76.08113981010335\n",
      "Current iteration=145, the loss=70.56161307025184\n",
      "Current iteration=146, the loss=78.74574137331385\n",
      "Current iteration=147, the loss=78.80408557684844\n",
      "Current iteration=148, the loss=71.69551350278698\n",
      "Current iteration=149, the loss=71.93285118768861\n",
      "Current iteration=150, the loss=77.0998327201948\n",
      "Current iteration=151, the loss=89.88319518836343\n",
      "Current iteration=152, the loss=84.88945291089715\n",
      "Current iteration=153, the loss=85.1113902840815\n",
      "Current iteration=154, the loss=73.54431127337332\n",
      "Current iteration=155, the loss=66.37540156521565\n",
      "Current iteration=156, the loss=76.61901350939182\n",
      "Current iteration=157, the loss=85.03734890898616\n",
      "Current iteration=158, the loss=70.7908390729906\n",
      "Current iteration=159, the loss=81.89127897487879\n",
      "Current iteration=160, the loss=73.33439845887665\n",
      "Current iteration=161, the loss=82.17187625612675\n",
      "Current iteration=162, the loss=76.0024286094452\n",
      "Current iteration=163, the loss=69.89116135581634\n",
      "Current iteration=164, the loss=80.74572609911799\n",
      "Current iteration=165, the loss=74.88455757010489\n",
      "Current iteration=166, the loss=70.45284189312162\n",
      "Current iteration=167, the loss=75.2317504496992\n",
      "Current iteration=168, the loss=81.07824510103774\n",
      "Current iteration=169, the loss=74.21517469990181\n",
      "Current iteration=170, the loss=85.9649393588204\n",
      "Current iteration=171, the loss=73.10577828193871\n",
      "Current iteration=172, the loss=65.62445926194303\n",
      "Current iteration=173, the loss=80.14366888102558\n",
      "Current iteration=174, the loss=63.60128871160251\n",
      "Current iteration=175, the loss=69.12014306645375\n",
      "Current iteration=176, the loss=69.52761432239173\n",
      "Current iteration=177, the loss=78.67299413325185\n",
      "Current iteration=178, the loss=83.31456928020813\n",
      "Current iteration=179, the loss=75.17974932102122\n",
      "Current iteration=180, the loss=78.13503289616449\n",
      "Current iteration=181, the loss=80.57840091988592\n",
      "Current iteration=182, the loss=71.97185335426386\n",
      "Current iteration=183, the loss=70.94140367416962\n",
      "Current iteration=184, the loss=87.74422545745553\n",
      "Current iteration=185, the loss=69.09365916152716\n",
      "Current iteration=186, the loss=76.97731825041548\n",
      "Current iteration=187, the loss=84.89671464267812\n",
      "Current iteration=188, the loss=75.72696861150217\n",
      "Current iteration=189, the loss=72.67208088573815\n",
      "Current iteration=190, the loss=70.98191795316538\n",
      "Current iteration=191, the loss=71.52857669775148\n",
      "Current iteration=192, the loss=75.08430337990274\n",
      "Current iteration=193, the loss=82.81990019482384\n",
      "Current iteration=194, the loss=79.92222768843354\n",
      "Current iteration=195, the loss=82.60248818493412\n",
      "Current iteration=196, the loss=69.40781407393143\n",
      "Current iteration=197, the loss=74.68691724064612\n",
      "Current iteration=198, the loss=77.6032092803458\n",
      "Current iteration=199, the loss=80.8386355922704\n",
      "Current iteration=200, the loss=81.7819504291061\n",
      "Current iteration=201, the loss=78.08055144719516\n",
      "Current iteration=202, the loss=62.46079709662249\n",
      "Current iteration=203, the loss=83.69601182964944\n",
      "Current iteration=204, the loss=71.79211295068521\n",
      "Current iteration=205, the loss=74.59753436401164\n",
      "Current iteration=206, the loss=63.335684222787904\n",
      "Current iteration=207, the loss=78.86541694512313\n",
      "Current iteration=208, the loss=72.66858035445607\n",
      "Current iteration=209, the loss=79.74175614642351\n",
      "Current iteration=210, the loss=75.54472238047053\n",
      "Current iteration=211, the loss=76.52909079752692\n",
      "Current iteration=212, the loss=77.32278879478895\n",
      "Current iteration=213, the loss=79.91346489077236\n",
      "Current iteration=214, the loss=81.9025136612865\n",
      "Current iteration=215, the loss=81.20188611554047\n",
      "Current iteration=216, the loss=66.90775699429767\n",
      "Current iteration=217, the loss=65.49691308291199\n",
      "Current iteration=218, the loss=85.37404322127034\n",
      "Current iteration=219, the loss=84.5512287674325\n",
      "Current iteration=220, the loss=74.43831515772318\n",
      "Current iteration=221, the loss=74.41722905625886\n",
      "Current iteration=222, the loss=78.76565978295429\n",
      "Current iteration=223, the loss=82.52209987376371\n",
      "Current iteration=224, the loss=75.91735799898589\n",
      "Current iteration=225, the loss=88.740126452352\n",
      "Current iteration=226, the loss=82.12108676589824\n",
      "Current iteration=227, the loss=79.06776483234346\n",
      "Current iteration=228, the loss=74.83966775146467\n",
      "Current iteration=229, the loss=79.34936574697664\n",
      "Current iteration=230, the loss=79.1389900957677\n",
      "Current iteration=231, the loss=74.28993386088108\n",
      "Current iteration=232, the loss=82.03283211889911\n",
      "Current iteration=233, the loss=78.86626647107181\n",
      "Current iteration=234, the loss=74.38331496280806\n",
      "Current iteration=235, the loss=78.82095414861\n",
      "Current iteration=236, the loss=73.0840476145729\n",
      "Current iteration=237, the loss=86.01042510770716\n",
      "Current iteration=238, the loss=69.26304382766793\n",
      "Current iteration=239, the loss=79.73417217853452\n",
      "Current iteration=240, the loss=76.31672874639065\n",
      "Current iteration=241, the loss=81.11554880361426\n",
      "Current iteration=242, the loss=69.49273066150073\n",
      "Current iteration=243, the loss=92.85056203418523\n",
      "Current iteration=244, the loss=73.59929936370096\n",
      "Current iteration=245, the loss=66.98211144800382\n",
      "Current iteration=246, the loss=72.73713961031658\n",
      "Current iteration=247, the loss=78.55054284813204\n",
      "Current iteration=248, the loss=82.3903207337467\n",
      "Current iteration=249, the loss=69.5622795045725\n",
      "Current iteration=250, the loss=79.34848827106569\n",
      "Current iteration=251, the loss=69.240070951783\n",
      "Current iteration=252, the loss=80.98960034012055\n",
      "Current iteration=253, the loss=77.86030440117497\n",
      "Current iteration=254, the loss=71.43806892138258\n",
      "Current iteration=255, the loss=69.40985530241377\n",
      "Current iteration=256, the loss=81.8308620645241\n",
      "Current iteration=257, the loss=70.53320923004003\n",
      "Current iteration=258, the loss=74.46159991797325\n",
      "Current iteration=259, the loss=77.53388109384898\n",
      "Current iteration=260, the loss=91.59891636392713\n",
      "Current iteration=261, the loss=70.57058306725779\n",
      "Current iteration=262, the loss=82.85109984977561\n",
      "Current iteration=263, the loss=81.35721130898003\n",
      "Current iteration=264, the loss=81.1516531119099\n",
      "Current iteration=265, the loss=76.4343619799125\n",
      "Current iteration=266, the loss=77.38952851763884\n",
      "Current iteration=267, the loss=87.29465467888511\n",
      "Current iteration=268, the loss=72.15554795372253\n",
      "Current iteration=269, the loss=75.99919584786083\n",
      "Current iteration=270, the loss=83.51426394849608\n",
      "Current iteration=271, the loss=69.0497953721796\n",
      "Current iteration=272, the loss=75.73131434985962\n",
      "Current iteration=273, the loss=76.54610612058804\n",
      "Current iteration=274, the loss=70.16262667365397\n",
      "Current iteration=275, the loss=67.15159741508015\n",
      "Current iteration=276, the loss=76.13150288469006\n",
      "Current iteration=277, the loss=75.0382912219272\n",
      "Current iteration=278, the loss=73.92232004668924\n",
      "Current iteration=279, the loss=70.44033994059546\n",
      "Current iteration=280, the loss=82.54504551842604\n",
      "Current iteration=281, the loss=65.83350788811438\n",
      "Current iteration=282, the loss=85.00780987012519\n",
      "Current iteration=283, the loss=75.67988650090113\n",
      "Current iteration=284, the loss=72.66387860451454\n",
      "Current iteration=285, the loss=75.42999021955836\n",
      "Current iteration=286, the loss=79.32554839883696\n",
      "Current iteration=287, the loss=75.08538716147788\n",
      "Current iteration=288, the loss=83.50967559150914\n",
      "Current iteration=289, the loss=69.00291414939781\n",
      "Current iteration=290, the loss=73.53869066865664\n",
      "Current iteration=291, the loss=70.70130351447823\n",
      "Current iteration=292, the loss=74.79499300224457\n",
      "Current iteration=293, the loss=78.12363879543648\n",
      "Current iteration=294, the loss=66.70463678208496\n",
      "Current iteration=295, the loss=70.14160730759423\n",
      "Current iteration=296, the loss=77.54402817587714\n",
      "Current iteration=297, the loss=80.24124127491824\n",
      "Current iteration=298, the loss=77.7052559863285\n",
      "Current iteration=299, the loss=65.04830099014588\n",
      "Current iteration=300, the loss=67.10252261492647\n",
      "Current iteration=301, the loss=76.98673271531158\n",
      "Current iteration=302, the loss=66.47939292338745\n",
      "Current iteration=303, the loss=65.3875400853999\n",
      "Current iteration=304, the loss=75.8152050585827\n",
      "Current iteration=305, the loss=63.66826300083424\n",
      "Current iteration=306, the loss=82.3512074626224\n",
      "Current iteration=307, the loss=76.62870305487698\n",
      "Current iteration=308, the loss=77.93718353278746\n",
      "Current iteration=309, the loss=74.77039790380596\n",
      "Current iteration=310, the loss=73.92905169107843\n",
      "Current iteration=311, the loss=77.90097824764968\n",
      "Current iteration=312, the loss=86.76028692933266\n",
      "Current iteration=313, the loss=83.62810291187591\n",
      "Current iteration=314, the loss=79.72279601662953\n",
      "Current iteration=315, the loss=73.92133698000903\n",
      "Current iteration=316, the loss=81.64474576738067\n",
      "Current iteration=317, the loss=78.27486605491063\n",
      "Current iteration=318, the loss=84.52425657673572\n",
      "Current iteration=319, the loss=87.35885674526855\n",
      "Current iteration=320, the loss=79.21959088534558\n",
      "Current iteration=321, the loss=71.81297466294194\n",
      "Current iteration=322, the loss=84.3335249845618\n",
      "Current iteration=323, the loss=71.93434013887426\n",
      "Current iteration=324, the loss=87.70186530368166\n",
      "Current iteration=325, the loss=87.39158139585876\n",
      "Current iteration=326, the loss=68.99496713785086\n",
      "Current iteration=327, the loss=76.31596098887003\n",
      "Current iteration=328, the loss=76.082768939646\n",
      "Current iteration=329, the loss=68.40048459328317\n",
      "Current iteration=330, the loss=70.03365798996263\n",
      "Current iteration=331, the loss=69.22163905829424\n",
      "Current iteration=332, the loss=75.8162409092111\n",
      "Current iteration=333, the loss=73.53086471893323\n",
      "Current iteration=334, the loss=81.72913483197804\n",
      "Current iteration=335, the loss=75.7993206760286\n",
      "Current iteration=336, the loss=82.46655906154763\n",
      "Current iteration=337, the loss=76.36817386904508\n",
      "Current iteration=338, the loss=73.58448304017458\n",
      "Current iteration=339, the loss=68.74834104125244\n",
      "Current iteration=340, the loss=74.36420185219964\n",
      "Current iteration=341, the loss=77.14008080684181\n",
      "Current iteration=342, the loss=76.95123445682769\n",
      "Current iteration=343, the loss=65.39451941566057\n",
      "Current iteration=344, the loss=73.35060333735338\n",
      "Current iteration=345, the loss=69.63856302747055\n",
      "Current iteration=346, the loss=77.01316465865904\n",
      "Current iteration=347, the loss=79.04136034246625\n",
      "Current iteration=348, the loss=82.90256090939215\n",
      "Current iteration=349, the loss=81.82696655482412\n",
      "Current iteration=350, the loss=69.89921196617944\n",
      "Current iteration=351, the loss=74.7777435824754\n",
      "Current iteration=352, the loss=84.83228630687465\n",
      "Current iteration=353, the loss=73.55996549804647\n",
      "Current iteration=354, the loss=70.14965532187608\n",
      "Current iteration=355, the loss=76.14084378963807\n",
      "Current iteration=356, the loss=76.90016092250008\n",
      "Current iteration=357, the loss=68.69522523443707\n",
      "Current iteration=358, the loss=79.49335415786118\n",
      "Current iteration=359, the loss=67.42421256639417\n",
      "Current iteration=360, the loss=71.97405216888049\n",
      "Current iteration=361, the loss=69.50152026029279\n",
      "Current iteration=362, the loss=68.97997723850261\n",
      "Current iteration=363, the loss=63.915496606694\n",
      "Current iteration=364, the loss=72.77038271612358\n",
      "Current iteration=365, the loss=75.98287545806568\n",
      "Current iteration=366, the loss=71.5103545707483\n",
      "Current iteration=367, the loss=80.63567692364748\n",
      "Current iteration=368, the loss=83.47312659494585\n",
      "Current iteration=369, the loss=86.18733418098569\n",
      "Current iteration=370, the loss=65.36221132597787\n",
      "Current iteration=371, the loss=78.15983090301103\n",
      "Current iteration=372, the loss=81.0683879081589\n",
      "Current iteration=373, the loss=80.48847111724429\n",
      "Current iteration=374, the loss=88.2841132172384\n",
      "Current iteration=375, the loss=75.90278638682344\n",
      "Current iteration=376, the loss=96.86702228136443\n",
      "Current iteration=377, the loss=74.940956768495\n",
      "Current iteration=378, the loss=74.02413861592446\n",
      "Current iteration=379, the loss=77.04729546014647\n",
      "Current iteration=380, the loss=68.06931893819807\n",
      "Current iteration=381, the loss=78.65999431504292\n",
      "Current iteration=382, the loss=74.89777482556772\n",
      "Current iteration=383, the loss=70.3172994582437\n",
      "Current iteration=384, the loss=67.47718320370646\n",
      "Current iteration=385, the loss=81.8364746485477\n",
      "Current iteration=386, the loss=77.90327900614471\n",
      "Current iteration=387, the loss=78.43671426883301\n",
      "Current iteration=388, the loss=87.07092518097082\n",
      "Current iteration=389, the loss=83.66981130724335\n",
      "Current iteration=390, the loss=89.48155257511667\n",
      "Current iteration=391, the loss=72.60993608550444\n",
      "Current iteration=392, the loss=73.2125568334624\n",
      "Current iteration=393, the loss=73.15275627057795\n",
      "Current iteration=394, the loss=77.06240100847367\n",
      "Current iteration=395, the loss=88.05443412994683\n",
      "Current iteration=396, the loss=70.08891278512952\n",
      "Current iteration=397, the loss=85.411269190936\n",
      "Current iteration=398, the loss=81.92656911862059\n",
      "Current iteration=399, the loss=69.2962642210252\n",
      "Current iteration=400, the loss=70.13419468319432\n",
      "Current iteration=401, the loss=74.20863519323865\n",
      "Current iteration=402, the loss=71.13157432773667\n",
      "Current iteration=403, the loss=80.3609008271545\n",
      "Current iteration=404, the loss=69.5737144408975\n",
      "Current iteration=405, the loss=74.3938139063433\n",
      "Current iteration=406, the loss=87.97007030763368\n",
      "Current iteration=407, the loss=81.00990548531374\n",
      "Current iteration=408, the loss=84.34763933516861\n",
      "Current iteration=409, the loss=73.1742321574589\n",
      "Current iteration=410, the loss=86.97115739349553\n",
      "Current iteration=411, the loss=85.3303130512335\n",
      "Current iteration=412, the loss=71.10551535302247\n",
      "Current iteration=413, the loss=65.42823757815262\n",
      "Current iteration=414, the loss=78.88879559758107\n",
      "Current iteration=415, the loss=77.41498331201898\n",
      "Current iteration=416, the loss=73.96874367476656\n",
      "Current iteration=417, the loss=73.01559174988785\n",
      "Current iteration=418, the loss=73.49606757905102\n",
      "Current iteration=419, the loss=71.38503121099552\n",
      "Current iteration=420, the loss=71.24888058392887\n",
      "Current iteration=421, the loss=76.11787336318511\n",
      "Current iteration=422, the loss=80.9124678879443\n",
      "Current iteration=423, the loss=79.25407421785837\n",
      "Current iteration=424, the loss=75.81771577952222\n",
      "Current iteration=425, the loss=72.77824567570033\n",
      "Current iteration=426, the loss=63.26036345023738\n",
      "Current iteration=427, the loss=73.24963552027427\n",
      "Current iteration=428, the loss=71.07683016517498\n",
      "Current iteration=429, the loss=76.3050127810175\n",
      "Current iteration=430, the loss=90.79949212464386\n",
      "Current iteration=431, the loss=64.94749634749354\n",
      "Current iteration=432, the loss=78.92897960041302\n",
      "Current iteration=433, the loss=63.13982971195693\n",
      "Current iteration=434, the loss=74.28004634289755\n",
      "Current iteration=435, the loss=72.55944317188465\n",
      "Current iteration=436, the loss=78.87645338442687\n",
      "Current iteration=437, the loss=77.4892000888418\n",
      "Current iteration=438, the loss=74.22604999664333\n",
      "Current iteration=439, the loss=92.08667282237384\n",
      "Current iteration=440, the loss=74.43355331854897\n",
      "Current iteration=441, the loss=73.80744919203184\n",
      "Current iteration=442, the loss=75.34378457757838\n",
      "Current iteration=443, the loss=85.22584663641629\n",
      "Current iteration=444, the loss=66.80452338784261\n",
      "Current iteration=445, the loss=77.80770550081715\n",
      "Current iteration=446, the loss=78.18118263265848\n",
      "Current iteration=447, the loss=81.85382722591768\n",
      "Current iteration=448, the loss=80.24225522740733\n",
      "Current iteration=449, the loss=81.64731668207858\n",
      "Current iteration=450, the loss=71.35141080484726\n",
      "Current iteration=451, the loss=73.81720200760958\n",
      "Current iteration=452, the loss=68.72164405086733\n",
      "Current iteration=453, the loss=76.54034178402492\n",
      "Current iteration=454, the loss=82.31869757184246\n",
      "Current iteration=455, the loss=72.82727146764348\n",
      "Current iteration=456, the loss=63.85477364996427\n",
      "Current iteration=457, the loss=82.85415062855665\n",
      "Current iteration=458, the loss=71.06216554911026\n",
      "Current iteration=459, the loss=67.80691388918899\n",
      "Current iteration=460, the loss=84.74113294259519\n",
      "Current iteration=461, the loss=70.91571168407404\n",
      "Current iteration=462, the loss=80.57542238668859\n",
      "Current iteration=463, the loss=70.88540251761746\n",
      "Current iteration=464, the loss=76.46301133414408\n",
      "Current iteration=465, the loss=77.8370228134071\n",
      "Current iteration=466, the loss=73.22160236176671\n",
      "Current iteration=467, the loss=78.65282687755624\n",
      "Current iteration=468, the loss=79.08442623470008\n",
      "Current iteration=469, the loss=86.59704548395104\n",
      "Current iteration=470, the loss=70.4563085130728\n",
      "Current iteration=471, the loss=77.8241715996873\n",
      "Current iteration=472, the loss=80.0390924519832\n",
      "Current iteration=473, the loss=84.23546712831352\n",
      "Current iteration=474, the loss=74.24516852706017\n",
      "Current iteration=475, the loss=65.57499056471187\n",
      "Current iteration=476, the loss=68.36966805806166\n",
      "Current iteration=477, the loss=70.95898046683033\n",
      "Current iteration=478, the loss=83.33281681100148\n",
      "Current iteration=479, the loss=67.62847672374329\n",
      "Current iteration=480, the loss=75.25661245728148\n",
      "Current iteration=481, the loss=77.88722506292544\n",
      "Current iteration=482, the loss=78.8625353681417\n",
      "Current iteration=483, the loss=81.2236242159563\n",
      "Current iteration=484, the loss=84.25475303476045\n",
      "Current iteration=485, the loss=81.25644491121679\n",
      "Current iteration=486, the loss=72.03309852009895\n",
      "Current iteration=487, the loss=79.08584904563763\n",
      "Current iteration=488, the loss=73.51746642646955\n",
      "Current iteration=489, the loss=80.82012979718522\n",
      "Current iteration=490, the loss=66.74017989452186\n",
      "Current iteration=491, the loss=80.01760418649351\n",
      "Current iteration=492, the loss=72.87089802702265\n",
      "Current iteration=493, the loss=67.6309554075522\n",
      "Current iteration=494, the loss=75.56434580441241\n",
      "Current iteration=495, the loss=68.13505041516318\n",
      "Current iteration=496, the loss=72.17007132608927\n",
      "Current iteration=497, the loss=76.89599994249255\n",
      "Current iteration=498, the loss=71.77598269119399\n",
      "Current iteration=499, the loss=73.68373730987294\n",
      "Current iteration=500, the loss=71.04382410865236\n",
      "Current iteration=501, the loss=76.52409427351259\n",
      "Current iteration=502, the loss=77.50012513523262\n",
      "Current iteration=503, the loss=77.40121372902622\n",
      "Current iteration=504, the loss=75.96943860969009\n",
      "Current iteration=505, the loss=75.3679866068236\n",
      "Current iteration=506, the loss=80.68707962101803\n",
      "Current iteration=507, the loss=75.6476353737923\n",
      "Current iteration=508, the loss=72.0552206689473\n",
      "Current iteration=509, the loss=74.77130378872081\n",
      "Current iteration=510, the loss=72.80653317184672\n",
      "Current iteration=511, the loss=76.41535830708214\n",
      "Current iteration=512, the loss=76.9943609653012\n",
      "Current iteration=513, the loss=74.76215009875634\n",
      "Current iteration=514, the loss=76.51145794502378\n",
      "Current iteration=515, the loss=78.47953958453054\n",
      "Current iteration=516, the loss=88.27048935717738\n",
      "Current iteration=517, the loss=79.78680085731389\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=518, the loss=66.03159230336894\n",
      "Current iteration=519, the loss=70.92316965655382\n",
      "Current iteration=520, the loss=74.17092694864174\n",
      "Current iteration=521, the loss=83.16223081586581\n",
      "Current iteration=522, the loss=70.92043998599212\n",
      "Current iteration=523, the loss=71.29863527303436\n",
      "Current iteration=524, the loss=79.85098855379466\n",
      "Current iteration=525, the loss=82.41469348562737\n",
      "Current iteration=526, the loss=70.53275436181698\n",
      "Current iteration=527, the loss=82.64959075988638\n",
      "Current iteration=528, the loss=60.277082231479056\n",
      "Current iteration=529, the loss=71.9507293670072\n",
      "Current iteration=530, the loss=91.99573074512695\n",
      "Current iteration=531, the loss=68.84024079093496\n",
      "Current iteration=532, the loss=72.91035400836313\n",
      "Current iteration=533, the loss=74.98539254948335\n",
      "Current iteration=534, the loss=73.79219718058144\n",
      "Current iteration=535, the loss=73.64502861686586\n",
      "Current iteration=536, the loss=76.13678817211587\n",
      "Current iteration=537, the loss=70.22637751556942\n",
      "Current iteration=538, the loss=83.27345325105263\n",
      "Current iteration=539, the loss=78.87729921559948\n",
      "Current iteration=540, the loss=78.52466219288044\n",
      "Current iteration=541, the loss=72.40170968970898\n",
      "Current iteration=542, the loss=71.50909121379374\n",
      "Current iteration=543, the loss=80.77604842129848\n",
      "Current iteration=544, the loss=78.52979667241354\n",
      "Current iteration=545, the loss=63.50153803303766\n",
      "Current iteration=546, the loss=76.95789516774167\n",
      "Current iteration=547, the loss=80.6120637285992\n",
      "Current iteration=548, the loss=84.6752581987418\n",
      "Current iteration=549, the loss=76.88423001371311\n",
      "Current iteration=550, the loss=66.3041425703251\n",
      "Current iteration=551, the loss=71.95202228247709\n",
      "Current iteration=552, the loss=77.98445760527426\n",
      "Current iteration=553, the loss=92.2086846090651\n",
      "Current iteration=554, the loss=73.65728792013101\n",
      "Current iteration=555, the loss=76.58348273730724\n",
      "Current iteration=556, the loss=74.02642005684447\n",
      "Current iteration=557, the loss=83.91923408456883\n",
      "Current iteration=558, the loss=78.30187846075833\n",
      "Current iteration=559, the loss=63.96993332216451\n",
      "Current iteration=560, the loss=73.44996476650343\n",
      "Current iteration=561, the loss=71.7453016210787\n",
      "Current iteration=562, the loss=76.63338303391009\n",
      "Current iteration=563, the loss=69.90481386471674\n",
      "Current iteration=564, the loss=70.45538777490023\n",
      "Current iteration=565, the loss=70.24021753343858\n",
      "Current iteration=566, the loss=69.97334926882951\n",
      "Current iteration=567, the loss=64.13906863328246\n",
      "Current iteration=568, the loss=73.94168976160307\n",
      "Current iteration=569, the loss=81.2344996729231\n",
      "Current iteration=570, the loss=66.55152226765749\n",
      "Current iteration=571, the loss=67.99984243879825\n",
      "Current iteration=572, the loss=72.53870681732016\n",
      "Current iteration=573, the loss=76.02747374088437\n",
      "Current iteration=574, the loss=70.14279737717277\n",
      "Current iteration=575, the loss=80.8047862393417\n",
      "Current iteration=576, the loss=79.08066745233133\n",
      "Current iteration=577, the loss=62.965308027856125\n",
      "Current iteration=578, the loss=71.50141091236746\n",
      "Current iteration=579, the loss=76.18464652470925\n",
      "Current iteration=580, the loss=83.12039267533295\n",
      "Current iteration=581, the loss=68.35874386652794\n",
      "Current iteration=582, the loss=66.19335685170884\n",
      "Current iteration=583, the loss=69.09610139259175\n",
      "Current iteration=584, the loss=76.51887143666397\n",
      "Current iteration=585, the loss=70.65366476876096\n",
      "Current iteration=586, the loss=81.7686151769742\n",
      "Current iteration=587, the loss=75.40232073029475\n",
      "Current iteration=588, the loss=82.1958216186029\n",
      "Current iteration=589, the loss=68.97823023631909\n",
      "Current iteration=590, the loss=79.76354060646858\n",
      "Current iteration=591, the loss=76.90036372482442\n",
      "Current iteration=592, the loss=80.66780245661624\n",
      "Current iteration=593, the loss=77.71336340010254\n",
      "Current iteration=594, the loss=66.5714059574893\n",
      "Current iteration=595, the loss=79.8361287489782\n",
      "Current iteration=596, the loss=84.85324130005131\n",
      "Current iteration=597, the loss=73.86461833398504\n",
      "Current iteration=598, the loss=82.70273827412362\n",
      "Current iteration=599, the loss=74.43647347786109\n",
      "Current iteration=600, the loss=76.9807329509861\n",
      "Current iteration=601, the loss=77.5866392791976\n",
      "Current iteration=602, the loss=70.09487410036745\n",
      "Current iteration=603, the loss=73.29446970257271\n",
      "Current iteration=604, the loss=80.49266276463857\n",
      "Current iteration=605, the loss=73.4536801163716\n",
      "Current iteration=606, the loss=70.4442546372213\n",
      "Current iteration=607, the loss=73.26209289476809\n",
      "Current iteration=608, the loss=79.6769634053553\n",
      "Current iteration=609, the loss=76.2398296072219\n",
      "Current iteration=610, the loss=79.92766351619662\n",
      "Current iteration=611, the loss=74.67507033690035\n",
      "Current iteration=612, the loss=66.17375449528812\n",
      "Current iteration=613, the loss=77.87082511726834\n",
      "Current iteration=614, the loss=72.00794605991172\n",
      "Current iteration=615, the loss=80.8911953274015\n",
      "Current iteration=616, the loss=68.63071130983884\n",
      "Current iteration=617, the loss=75.24501283528696\n",
      "Current iteration=618, the loss=76.35120386080982\n",
      "Current iteration=619, the loss=65.64859472007343\n",
      "Current iteration=620, the loss=75.420973394915\n",
      "Current iteration=621, the loss=78.99796635029372\n",
      "Current iteration=622, the loss=75.61850782739799\n",
      "Current iteration=623, the loss=66.0377635468962\n",
      "Current iteration=624, the loss=70.74408557632515\n",
      "Current iteration=625, the loss=78.14248066955196\n",
      "Current iteration=626, the loss=74.00898236987356\n",
      "Current iteration=627, the loss=78.54740353888582\n",
      "Current iteration=628, the loss=76.93568394085509\n",
      "Current iteration=629, the loss=66.97639561878466\n",
      "Current iteration=630, the loss=70.91004864929023\n",
      "Current iteration=631, the loss=79.04615027679526\n",
      "Current iteration=632, the loss=82.50266585582284\n",
      "Current iteration=633, the loss=73.19508804173353\n",
      "Current iteration=634, the loss=81.26457223547393\n",
      "Current iteration=635, the loss=62.7285654549885\n",
      "Current iteration=636, the loss=68.34148944199048\n",
      "Current iteration=637, the loss=64.24439121067601\n",
      "Current iteration=638, the loss=71.23260920381878\n",
      "Current iteration=639, the loss=84.4442247498518\n",
      "Current iteration=640, the loss=75.38595506263361\n",
      "Current iteration=641, the loss=81.21118868887834\n",
      "Current iteration=642, the loss=74.82505281997038\n",
      "Current iteration=643, the loss=72.58607326392791\n",
      "Current iteration=644, the loss=80.2004754800935\n",
      "Current iteration=645, the loss=67.29202748569072\n",
      "Current iteration=646, the loss=69.71161614275888\n",
      "Current iteration=647, the loss=76.96789642980256\n",
      "Current iteration=648, the loss=79.84919546128401\n",
      "Current iteration=649, the loss=82.11307436114828\n",
      "Current iteration=650, the loss=80.12199309741978\n",
      "Current iteration=651, the loss=74.36905833467856\n",
      "Current iteration=652, the loss=67.93065639703121\n",
      "Current iteration=653, the loss=72.3617506568109\n",
      "Current iteration=654, the loss=74.5630950580987\n",
      "Current iteration=655, the loss=75.00282418861889\n",
      "Current iteration=656, the loss=76.80659349862677\n",
      "Current iteration=657, the loss=78.04364118627282\n",
      "Current iteration=658, the loss=68.90410792743809\n",
      "Current iteration=659, the loss=80.441874462225\n",
      "Current iteration=660, the loss=81.73809781921662\n",
      "Current iteration=661, the loss=69.71562559940293\n",
      "Current iteration=662, the loss=73.9086399829273\n",
      "Current iteration=663, the loss=79.37907940462438\n",
      "Current iteration=664, the loss=70.68926484892746\n",
      "Current iteration=665, the loss=75.98781335166098\n",
      "Current iteration=666, the loss=70.72127231221491\n",
      "Current iteration=667, the loss=79.37979304171014\n",
      "Current iteration=668, the loss=75.88671911423467\n",
      "Current iteration=669, the loss=70.95512501152072\n",
      "Current iteration=670, the loss=66.16619400417196\n",
      "Current iteration=671, the loss=73.93022592712826\n",
      "Current iteration=672, the loss=93.01738129715312\n",
      "Current iteration=673, the loss=78.36430802569839\n",
      "Current iteration=674, the loss=74.88300956281225\n",
      "Current iteration=675, the loss=67.44180079119687\n",
      "Current iteration=676, the loss=65.6867062353835\n",
      "Current iteration=677, the loss=85.05008010716313\n",
      "Current iteration=678, the loss=79.21496820925307\n",
      "Current iteration=679, the loss=72.5102591000831\n",
      "Current iteration=680, the loss=82.44765753344328\n",
      "Current iteration=681, the loss=72.97092729520371\n",
      "Current iteration=682, the loss=85.36519878272276\n",
      "Current iteration=683, the loss=72.41954658922472\n",
      "Current iteration=684, the loss=83.97863645310885\n",
      "Current iteration=685, the loss=78.47775498517942\n",
      "Current iteration=686, the loss=80.29179158973633\n",
      "Current iteration=687, the loss=75.58271577984584\n",
      "Current iteration=688, the loss=74.83709192384421\n",
      "Current iteration=689, the loss=74.16859270308944\n",
      "Current iteration=690, the loss=73.02869012697067\n",
      "Current iteration=691, the loss=81.47724458928124\n",
      "Current iteration=692, the loss=82.07399935454406\n",
      "Current iteration=693, the loss=74.32459070953414\n",
      "Current iteration=694, the loss=77.3157504279281\n",
      "Current iteration=695, the loss=76.60444958251679\n",
      "Current iteration=696, the loss=86.09542390454118\n",
      "Current iteration=697, the loss=76.3847706081934\n",
      "Current iteration=698, the loss=72.99697435662065\n",
      "Current iteration=699, the loss=75.5209605606583\n",
      "Current iteration=700, the loss=76.50606684105927\n",
      "Current iteration=701, the loss=78.95032569825199\n",
      "Current iteration=702, the loss=66.70756505559652\n",
      "Current iteration=703, the loss=78.80810503584866\n",
      "Current iteration=704, the loss=69.14328723597457\n",
      "Current iteration=705, the loss=83.09508637495017\n",
      "Current iteration=706, the loss=70.04875400150488\n",
      "Current iteration=707, the loss=71.93757176122838\n",
      "Current iteration=708, the loss=74.55421319379266\n",
      "Current iteration=709, the loss=74.36497215304185\n",
      "Current iteration=710, the loss=70.42724766472759\n",
      "Current iteration=711, the loss=70.50865925633141\n",
      "Current iteration=712, the loss=72.27363278678366\n",
      "Current iteration=713, the loss=75.0534813394797\n",
      "Current iteration=714, the loss=81.9367721753605\n",
      "Current iteration=715, the loss=83.2762491208751\n",
      "Current iteration=716, the loss=70.31652600961345\n",
      "Current iteration=717, the loss=72.28999920316663\n",
      "Current iteration=718, the loss=70.21266937951582\n",
      "Current iteration=719, the loss=70.12989849997625\n",
      "Current iteration=720, the loss=80.77080798439289\n",
      "Current iteration=721, the loss=68.241540182279\n",
      "Current iteration=722, the loss=83.24206588669696\n",
      "Current iteration=723, the loss=82.63311638085963\n",
      "Current iteration=724, the loss=74.98763624939097\n",
      "Current iteration=725, the loss=71.9035303554723\n",
      "Current iteration=726, the loss=74.95039246872182\n",
      "Current iteration=727, the loss=88.62508293685012\n",
      "Current iteration=728, the loss=77.01072754870228\n",
      "Current iteration=729, the loss=76.20454894209296\n",
      "Current iteration=730, the loss=75.8221420853001\n",
      "Current iteration=731, the loss=90.4803348543216\n",
      "Current iteration=732, the loss=68.56846369302573\n",
      "Current iteration=733, the loss=84.92856943989813\n",
      "Current iteration=734, the loss=70.04363489658871\n",
      "Current iteration=735, the loss=75.30648611430759\n",
      "Current iteration=736, the loss=80.93181138378033\n",
      "Current iteration=737, the loss=79.94450064650374\n",
      "Current iteration=738, the loss=78.03168778249942\n",
      "Current iteration=739, the loss=83.55660563751971\n",
      "Current iteration=740, the loss=75.16228638077045\n",
      "Current iteration=741, the loss=72.67381746627714\n",
      "Current iteration=742, the loss=74.41739542577236\n",
      "Current iteration=743, the loss=76.88807490280175\n",
      "Current iteration=744, the loss=86.45793419422489\n",
      "Current iteration=745, the loss=77.90605880916854\n",
      "Current iteration=746, the loss=75.42193637645363\n",
      "Current iteration=747, the loss=90.15181306529055\n",
      "Current iteration=748, the loss=76.02631152810056\n",
      "Current iteration=749, the loss=78.44812791070638\n",
      "Current iteration=750, the loss=77.21101137078489\n",
      "Current iteration=751, the loss=68.58412250095179\n",
      "Current iteration=752, the loss=73.9375649322314\n",
      "Current iteration=753, the loss=67.29141555157084\n",
      "Current iteration=754, the loss=76.5765711300647\n",
      "Current iteration=755, the loss=74.32778786608225\n",
      "Current iteration=756, the loss=63.11609485158031\n",
      "Current iteration=757, the loss=81.5377446259072\n",
      "Current iteration=758, the loss=76.72790516084322\n",
      "Current iteration=759, the loss=80.51548649782232\n",
      "Current iteration=760, the loss=78.36693741453645\n",
      "Current iteration=761, the loss=88.59198222728463\n",
      "Current iteration=762, the loss=84.71636293965594\n",
      "Current iteration=763, the loss=79.57462116655142\n",
      "Current iteration=764, the loss=67.61613117324087\n",
      "Current iteration=765, the loss=65.04935869614873\n",
      "Current iteration=766, the loss=70.97934542202637\n",
      "Current iteration=767, the loss=63.038059044901615\n",
      "Current iteration=768, the loss=74.48430982578716\n",
      "Current iteration=769, the loss=69.72066872137154\n",
      "Current iteration=770, the loss=76.82774489689245\n",
      "Current iteration=771, the loss=81.17792656384944\n",
      "Current iteration=772, the loss=76.71448035602735\n",
      "Current iteration=773, the loss=67.98601695175003\n",
      "Current iteration=774, the loss=73.58835394338374\n",
      "Current iteration=775, the loss=76.3681219255093\n",
      "Current iteration=776, the loss=78.70046741652794\n",
      "Current iteration=777, the loss=85.01936174008648\n",
      "Current iteration=778, the loss=73.62938523970791\n",
      "Current iteration=779, the loss=72.18109405716946\n",
      "Current iteration=780, the loss=69.05156112251801\n",
      "Current iteration=781, the loss=78.40535272810766\n",
      "Current iteration=782, the loss=71.42787323287635\n",
      "Current iteration=783, the loss=71.83445639086878\n",
      "Current iteration=784, the loss=61.019435808458375\n",
      "Current iteration=785, the loss=70.1744849047318\n",
      "Current iteration=786, the loss=71.46194588031497\n",
      "Current iteration=787, the loss=73.06003538010133\n",
      "Current iteration=788, the loss=82.25044447060071\n",
      "Current iteration=789, the loss=65.62926555419747\n",
      "Current iteration=790, the loss=69.79448418858684\n",
      "Current iteration=791, the loss=77.39592729895597\n",
      "Current iteration=792, the loss=78.54965292712527\n",
      "Current iteration=793, the loss=76.72060401856248\n",
      "Current iteration=794, the loss=78.9334704960861\n",
      "Current iteration=795, the loss=71.56403247073484\n",
      "Current iteration=796, the loss=81.57512986346974\n",
      "Current iteration=797, the loss=83.4288914185038\n",
      "Current iteration=798, the loss=76.25327759340867\n",
      "Current iteration=799, the loss=67.35241583512833\n",
      "Current iteration=800, the loss=68.67028436838703\n",
      "Current iteration=801, the loss=69.68640773549903\n",
      "Current iteration=802, the loss=74.07935327190825\n",
      "Current iteration=803, the loss=89.61208745883201\n",
      "Current iteration=804, the loss=90.62894688863173\n",
      "Current iteration=805, the loss=76.06385385474141\n",
      "Current iteration=806, the loss=77.09949656792188\n",
      "Current iteration=807, the loss=73.03155713106588\n",
      "Current iteration=808, the loss=70.34200840745353\n",
      "Current iteration=809, the loss=79.52110386871504\n",
      "Current iteration=810, the loss=60.98501942694476\n",
      "Current iteration=811, the loss=66.14788698134647\n",
      "Current iteration=812, the loss=64.16993509510837\n",
      "Current iteration=813, the loss=77.73412005399571\n",
      "Current iteration=814, the loss=72.15002069621423\n",
      "Current iteration=815, the loss=71.81390791949887\n",
      "Current iteration=816, the loss=79.37124321617767\n",
      "Current iteration=817, the loss=69.07440247988748\n",
      "Current iteration=818, the loss=73.79900085785981\n",
      "Current iteration=819, the loss=82.96869574407567\n",
      "Current iteration=820, the loss=74.06494936048279\n",
      "Current iteration=821, the loss=70.74082878340064\n",
      "Current iteration=822, the loss=64.49324033894027\n",
      "Current iteration=823, the loss=79.26626107435563\n",
      "Current iteration=824, the loss=79.02343848203233\n",
      "Current iteration=825, the loss=73.20621539933771\n",
      "Current iteration=826, the loss=83.1374160144978\n",
      "Current iteration=827, the loss=74.27606859914414\n",
      "Current iteration=828, the loss=88.0197342482465\n",
      "Current iteration=829, the loss=64.90288911881355\n",
      "Current iteration=830, the loss=76.6464207273304\n",
      "Current iteration=831, the loss=84.037877860474\n",
      "Current iteration=832, the loss=70.28469273965653\n",
      "Current iteration=833, the loss=69.2368870763392\n",
      "Current iteration=834, the loss=66.16828580367361\n",
      "Current iteration=835, the loss=71.74097618556105\n",
      "Current iteration=836, the loss=79.63551813275552\n",
      "Current iteration=837, the loss=64.26986699081274\n",
      "Current iteration=838, the loss=67.8689813553286\n",
      "Current iteration=839, the loss=59.54608557216602\n",
      "Current iteration=840, the loss=86.19974349685683\n",
      "Current iteration=841, the loss=72.47809917505215\n",
      "Current iteration=842, the loss=74.36216996897855\n",
      "Current iteration=843, the loss=68.45657375138386\n",
      "Current iteration=844, the loss=79.08740788570375\n",
      "Current iteration=845, the loss=71.28548147031236\n",
      "Current iteration=846, the loss=66.6263801012075\n",
      "Current iteration=847, the loss=87.02579200975782\n",
      "Current iteration=848, the loss=70.69755573885367\n",
      "Current iteration=849, the loss=75.41833750842005\n",
      "Current iteration=850, the loss=77.2590797880654\n",
      "Current iteration=851, the loss=77.44961740562877\n",
      "Current iteration=852, the loss=71.07213492126195\n",
      "Current iteration=853, the loss=74.4523141296127\n",
      "Current iteration=854, the loss=67.15830036618951\n",
      "Current iteration=855, the loss=86.65331707897245\n",
      "Current iteration=856, the loss=76.99729383745296\n",
      "Current iteration=857, the loss=67.05518219166204\n",
      "Current iteration=858, the loss=81.45077517808838\n",
      "Current iteration=859, the loss=66.70939596736173\n",
      "Current iteration=860, the loss=77.39622065083033\n",
      "Current iteration=861, the loss=74.10384034363892\n",
      "Current iteration=862, the loss=79.8703919487798\n",
      "Current iteration=863, the loss=79.3438532691866\n",
      "Current iteration=864, the loss=71.19718306899696\n",
      "Current iteration=865, the loss=76.52976930853231\n",
      "Current iteration=866, the loss=86.50063039065199\n",
      "Current iteration=867, the loss=82.97838811278382\n",
      "Current iteration=868, the loss=79.36124025964045\n",
      "Current iteration=869, the loss=63.570449421895475\n",
      "Current iteration=870, the loss=74.21936715795417\n",
      "Current iteration=871, the loss=68.50244525179077\n",
      "Current iteration=872, the loss=76.0619862725262\n",
      "Current iteration=873, the loss=69.98856628810313\n",
      "Current iteration=874, the loss=73.42550142373159\n",
      "Current iteration=875, the loss=72.27108507866515\n",
      "Current iteration=876, the loss=79.52704575644458\n",
      "Current iteration=877, the loss=76.5937161845229\n",
      "Current iteration=878, the loss=71.29511290796022\n",
      "Current iteration=879, the loss=69.81312618332424\n",
      "Current iteration=880, the loss=67.81587298994364\n",
      "Current iteration=881, the loss=81.64807142073604\n",
      "Current iteration=882, the loss=76.66033425156894\n",
      "Current iteration=883, the loss=67.24365840825847\n",
      "Current iteration=884, the loss=76.46363105957285\n",
      "Current iteration=885, the loss=72.15698266506247\n",
      "Current iteration=886, the loss=77.0784342970105\n",
      "Current iteration=887, the loss=76.93467549236746\n",
      "Current iteration=888, the loss=68.45259423748897\n",
      "Current iteration=889, the loss=87.78486952352512\n",
      "Current iteration=890, the loss=77.3626987116531\n",
      "Current iteration=891, the loss=65.62758344151669\n",
      "Current iteration=892, the loss=66.66762658923298\n",
      "Current iteration=893, the loss=83.02772862266897\n",
      "Current iteration=894, the loss=85.02081643963005\n",
      "Current iteration=895, the loss=90.5873603993386\n",
      "Current iteration=896, the loss=81.35899452189975\n",
      "Current iteration=897, the loss=79.96683999060951\n",
      "Current iteration=898, the loss=90.88219439057318\n",
      "Current iteration=899, the loss=75.46198879747874\n",
      "Current iteration=900, the loss=70.49683607757088\n",
      "Current iteration=901, the loss=84.912227871611\n",
      "Current iteration=902, the loss=66.07316619020976\n",
      "Current iteration=903, the loss=79.73686792396211\n",
      "Current iteration=904, the loss=82.58100326784552\n",
      "Current iteration=905, the loss=77.38091034576766\n",
      "Current iteration=906, the loss=78.53224552506973\n",
      "Current iteration=907, the loss=85.9531998503192\n",
      "Current iteration=908, the loss=76.21047922684792\n",
      "Current iteration=909, the loss=67.82953192885513\n",
      "Current iteration=910, the loss=71.20404058718123\n",
      "Current iteration=911, the loss=78.9031109496332\n",
      "Current iteration=912, the loss=78.86073268141182\n",
      "Current iteration=913, the loss=75.10733349874567\n",
      "Current iteration=914, the loss=87.55233890515245\n",
      "Current iteration=915, the loss=79.52470509336251\n",
      "Current iteration=916, the loss=84.30710308209896\n",
      "Current iteration=917, the loss=69.97773264476888\n",
      "Current iteration=918, the loss=77.38150710231177\n",
      "Current iteration=919, the loss=88.75848568268421\n",
      "Current iteration=920, the loss=78.02521783704256\n",
      "Current iteration=921, the loss=71.55629922730719\n",
      "Current iteration=922, the loss=79.29405721707805\n",
      "Current iteration=923, the loss=74.42035269481799\n",
      "Current iteration=924, the loss=83.6767693023622\n",
      "Current iteration=925, the loss=75.63787411442631\n",
      "Current iteration=926, the loss=77.94267843197926\n",
      "Current iteration=927, the loss=73.65840323475847\n",
      "Current iteration=928, the loss=68.57153174300419\n",
      "Current iteration=929, the loss=77.9548018735137\n",
      "Current iteration=930, the loss=80.17622792828351\n",
      "Current iteration=931, the loss=81.05474470844918\n",
      "Current iteration=932, the loss=74.57488770721659\n",
      "Current iteration=933, the loss=78.19255347268913\n",
      "Current iteration=934, the loss=78.26659399611819\n",
      "Current iteration=935, the loss=68.94128024735556\n",
      "Current iteration=936, the loss=60.56496471741623\n",
      "Current iteration=937, the loss=85.8721078311086\n",
      "Current iteration=938, the loss=81.27917722440833\n",
      "Current iteration=939, the loss=71.0450500690847\n",
      "Current iteration=940, the loss=74.60536733270905\n",
      "Current iteration=941, the loss=84.74072270612177\n",
      "Current iteration=942, the loss=78.39898290430281\n",
      "Current iteration=943, the loss=71.31590194489397\n",
      "Current iteration=944, the loss=65.81213576777446\n",
      "Current iteration=945, the loss=74.48497768609889\n",
      "Current iteration=946, the loss=79.77527875342747\n",
      "Current iteration=947, the loss=74.74590940168873\n",
      "Current iteration=948, the loss=72.03800213514845\n",
      "Current iteration=949, the loss=65.84658237633893\n",
      "Current iteration=950, the loss=69.83749016632022\n",
      "Current iteration=951, the loss=83.31902201937567\n",
      "Current iteration=952, the loss=64.34858053295912\n",
      "Current iteration=953, the loss=78.30258382407953\n",
      "Current iteration=954, the loss=61.175741254472044\n",
      "Current iteration=955, the loss=71.2374933380352\n",
      "Current iteration=956, the loss=91.0705493474754\n",
      "Current iteration=957, the loss=76.17862618808832\n",
      "Current iteration=958, the loss=84.22064095610699\n",
      "Current iteration=959, the loss=77.0252726112883\n",
      "Current iteration=960, the loss=68.68436857681014\n",
      "Current iteration=961, the loss=74.79957093292396\n",
      "Current iteration=962, the loss=70.74635972862964\n",
      "Current iteration=963, the loss=71.31247272836856\n",
      "Current iteration=964, the loss=89.76319831766543\n",
      "Current iteration=965, the loss=82.08813667236025\n",
      "Current iteration=966, the loss=75.54082338546192\n",
      "Current iteration=967, the loss=67.37826962455273\n",
      "Current iteration=968, the loss=73.9460284797606\n",
      "Current iteration=969, the loss=76.72022807439055\n",
      "Current iteration=970, the loss=78.99007597484868\n",
      "Current iteration=971, the loss=85.23115946688019\n",
      "Current iteration=972, the loss=90.26648304914605\n",
      "Current iteration=973, the loss=73.29667035634591\n",
      "Current iteration=974, the loss=79.4884306118039\n",
      "Current iteration=975, the loss=84.96712936040339\n",
      "Current iteration=976, the loss=69.9854953350585\n",
      "Current iteration=977, the loss=85.71238838070227\n",
      "Current iteration=978, the loss=75.21020987362546\n",
      "Current iteration=979, the loss=70.20711865156018\n",
      "Current iteration=980, the loss=74.96747724038387\n",
      "Current iteration=981, the loss=75.43477252002873\n",
      "Current iteration=982, the loss=66.65399117779955\n",
      "Current iteration=983, the loss=83.06960954738668\n",
      "Current iteration=984, the loss=83.19461256953913\n",
      "Current iteration=985, the loss=75.92992456733064\n",
      "Current iteration=986, the loss=61.02741748932257\n",
      "Current iteration=987, the loss=76.62563043209484\n",
      "Current iteration=988, the loss=79.09375676937405\n",
      "Current iteration=989, the loss=75.80916719790571\n",
      "Current iteration=990, the loss=73.34807429440747\n",
      "Current iteration=991, the loss=78.91088920608298\n",
      "Current iteration=992, the loss=82.5363214473751\n",
      "Current iteration=993, the loss=68.60557058507007\n",
      "Current iteration=994, the loss=82.40245210588932\n",
      "Current iteration=995, the loss=81.27465536177309\n",
      "Current iteration=996, the loss=69.85110028826122\n",
      "Current iteration=997, the loss=71.52358446564612\n",
      "Current iteration=998, the loss=61.68873300733466\n",
      "Current iteration=999, the loss=68.87702628831485\n",
      "Current iteration=1000, the loss=78.17840373207974\n",
      "Current iteration=1001, the loss=74.44663603672925\n",
      "Current iteration=1002, the loss=79.12897328020854\n",
      "Current iteration=1003, the loss=81.76375787855008\n",
      "Current iteration=1004, the loss=78.03738512350233\n",
      "Current iteration=1005, the loss=78.937888363865\n",
      "Current iteration=1006, the loss=86.98476366581087\n",
      "Current iteration=1007, the loss=79.67710813884608\n",
      "Current iteration=1008, the loss=70.37057442982257\n",
      "Current iteration=1009, the loss=72.10174138667756\n",
      "Current iteration=1010, the loss=76.20943340891671\n",
      "Current iteration=1011, the loss=64.43701849741478\n",
      "Current iteration=1012, the loss=88.34585014784544\n",
      "Current iteration=1013, the loss=73.27859149057954\n",
      "Current iteration=1014, the loss=73.4270897161741\n",
      "Current iteration=1015, the loss=78.26562651397884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=1016, the loss=72.42970344825915\n",
      "Current iteration=1017, the loss=74.9356209820235\n",
      "Current iteration=1018, the loss=90.32918484857177\n",
      "Current iteration=1019, the loss=65.64229161188456\n",
      "Current iteration=1020, the loss=71.81907738012669\n",
      "Current iteration=1021, the loss=86.90750754318421\n",
      "Current iteration=1022, the loss=71.8606479951358\n",
      "Current iteration=1023, the loss=65.96735991324194\n",
      "Current iteration=1024, the loss=67.13817523926542\n",
      "Current iteration=1025, the loss=76.98657143761321\n",
      "Current iteration=1026, the loss=69.41152549417995\n",
      "Current iteration=1027, the loss=75.59271893620641\n",
      "Current iteration=1028, the loss=73.80914428859941\n",
      "Current iteration=1029, the loss=75.18635231823032\n",
      "Current iteration=1030, the loss=86.38495429561667\n",
      "Current iteration=1031, the loss=72.5752449751799\n",
      "Current iteration=1032, the loss=66.59290911353646\n",
      "Current iteration=1033, the loss=75.55744552932735\n",
      "Current iteration=1034, the loss=63.865689070651946\n",
      "Current iteration=1035, the loss=80.61307905903584\n",
      "Current iteration=1036, the loss=71.34393646609442\n",
      "Current iteration=1037, the loss=60.022843880038934\n",
      "Current iteration=1038, the loss=72.00517151533518\n",
      "Current iteration=1039, the loss=81.66986057529628\n",
      "Current iteration=1040, the loss=67.3716865484077\n",
      "Current iteration=1041, the loss=70.80807628708855\n",
      "Current iteration=1042, the loss=72.89553374433257\n",
      "Current iteration=1043, the loss=64.16609648482498\n",
      "Current iteration=1044, the loss=79.60281740294423\n",
      "Current iteration=1045, the loss=76.11130333664607\n",
      "Current iteration=1046, the loss=76.95241853000178\n",
      "Current iteration=1047, the loss=71.1506581620952\n",
      "Current iteration=1048, the loss=60.10734307177445\n",
      "Current iteration=1049, the loss=70.52123134742453\n",
      "Current iteration=1050, the loss=63.26598320871415\n",
      "Current iteration=1051, the loss=56.53874968635705\n",
      "Current iteration=1052, the loss=80.1436045313238\n",
      "Current iteration=1053, the loss=83.37114078447064\n",
      "Current iteration=1054, the loss=69.14967694407194\n",
      "Current iteration=1055, the loss=76.84161301637619\n",
      "Current iteration=1056, the loss=71.84830645947963\n",
      "Current iteration=1057, the loss=74.36859244124197\n",
      "Current iteration=1058, the loss=72.60522465994464\n",
      "Current iteration=1059, the loss=72.14370406094591\n",
      "Current iteration=1060, the loss=64.13325176371777\n",
      "Current iteration=1061, the loss=79.58127558980058\n",
      "Current iteration=1062, the loss=91.81191925008882\n",
      "Current iteration=1063, the loss=70.14179258391918\n",
      "Current iteration=1064, the loss=83.78713101320807\n",
      "Current iteration=1065, the loss=73.29873869147774\n",
      "Current iteration=1066, the loss=73.94464119553518\n",
      "Current iteration=1067, the loss=82.88688213003957\n",
      "Current iteration=1068, the loss=82.25969756629345\n",
      "Current iteration=1069, the loss=79.67570197286668\n",
      "Current iteration=1070, the loss=69.1460525386861\n",
      "Current iteration=1071, the loss=78.0274124952505\n",
      "Current iteration=1072, the loss=69.33149556237923\n",
      "Current iteration=1073, the loss=76.87584885569558\n",
      "Current iteration=1074, the loss=77.58579121713208\n",
      "Current iteration=1075, the loss=73.52668521498939\n",
      "Current iteration=1076, the loss=87.99195834898114\n",
      "Current iteration=1077, the loss=70.41138347407019\n",
      "Current iteration=1078, the loss=72.28452691715422\n",
      "Current iteration=1079, the loss=66.35543880069504\n",
      "Current iteration=1080, the loss=82.59473813465021\n",
      "Current iteration=1081, the loss=77.5281712065582\n",
      "Current iteration=1082, the loss=80.67597098958588\n",
      "Current iteration=1083, the loss=81.30652543039832\n",
      "Current iteration=1084, the loss=80.05777595741424\n",
      "Current iteration=1085, the loss=82.44348337087584\n",
      "Current iteration=1086, the loss=72.23872315225086\n",
      "Current iteration=1087, the loss=71.12292099081685\n",
      "Current iteration=1088, the loss=66.03083624544917\n",
      "Current iteration=1089, the loss=74.47658104070953\n",
      "Current iteration=1090, the loss=70.0923216945998\n",
      "Current iteration=1091, the loss=77.74129605560447\n",
      "Current iteration=1092, the loss=71.54334366190908\n",
      "Current iteration=1093, the loss=74.39163433241583\n",
      "Current iteration=1094, the loss=72.14007338494423\n",
      "Current iteration=1095, the loss=83.67367839088868\n",
      "Current iteration=1096, the loss=64.1017245058585\n",
      "Current iteration=1097, the loss=77.85972879096656\n",
      "Current iteration=1098, the loss=78.00592022052533\n",
      "Current iteration=1099, the loss=86.68231785751996\n",
      "Current iteration=1100, the loss=83.48442854270898\n",
      "Current iteration=1101, the loss=73.74354300030913\n",
      "Current iteration=1102, the loss=67.98015990571452\n",
      "Current iteration=1103, the loss=82.29976290237147\n",
      "Current iteration=1104, the loss=72.43210759199567\n",
      "Current iteration=1105, the loss=82.78698198791437\n",
      "Current iteration=1106, the loss=73.61539632115729\n",
      "Current iteration=1107, the loss=71.49828871016334\n",
      "Current iteration=1108, the loss=73.8048642481044\n",
      "Current iteration=1109, the loss=90.77751065876761\n",
      "Current iteration=1110, the loss=67.11085784131632\n",
      "Current iteration=1111, the loss=74.46158564824023\n",
      "Current iteration=1112, the loss=80.31503150279053\n",
      "Current iteration=1113, the loss=83.31401480133854\n",
      "Current iteration=1114, the loss=75.99812359658242\n",
      "Current iteration=1115, the loss=80.68559908455032\n",
      "Current iteration=1116, the loss=68.2433488326117\n",
      "Current iteration=1117, the loss=65.55408477789345\n",
      "Current iteration=1118, the loss=76.12873412342523\n",
      "Current iteration=1119, the loss=73.84250700193445\n",
      "Current iteration=1120, the loss=66.32114107657911\n",
      "Current iteration=1121, the loss=78.37520496651297\n",
      "Current iteration=1122, the loss=72.51199781158596\n",
      "Current iteration=1123, the loss=67.0477281919108\n",
      "Current iteration=1124, the loss=76.78939123933162\n",
      "Current iteration=1125, the loss=73.38739538400336\n",
      "Current iteration=1126, the loss=75.21768949255667\n",
      "Current iteration=1127, the loss=80.66317567589316\n",
      "Current iteration=1128, the loss=72.13857066236505\n",
      "Current iteration=1129, the loss=72.51070464940636\n",
      "Current iteration=1130, the loss=82.58868418692845\n",
      "Current iteration=1131, the loss=76.17749734972926\n",
      "Current iteration=1132, the loss=73.05823376569859\n",
      "Current iteration=1133, the loss=75.1945956790168\n",
      "Current iteration=1134, the loss=71.85999788043259\n",
      "Current iteration=1135, the loss=71.77766768506893\n",
      "Current iteration=1136, the loss=76.6655810606668\n",
      "Current iteration=1137, the loss=74.29099031329108\n",
      "Current iteration=1138, the loss=70.88143053195631\n",
      "Current iteration=1139, the loss=77.66952465319733\n",
      "Current iteration=1140, the loss=75.64265544261855\n",
      "Current iteration=1141, the loss=77.68194077965286\n",
      "Current iteration=1142, the loss=81.3494094886525\n",
      "Current iteration=1143, the loss=70.51327289711638\n",
      "Current iteration=1144, the loss=74.4676223009202\n",
      "Current iteration=1145, the loss=72.42880609933269\n",
      "Current iteration=1146, the loss=64.36038509856778\n",
      "Current iteration=1147, the loss=79.48501975389757\n",
      "Current iteration=1148, the loss=68.4402949941129\n",
      "Current iteration=1149, the loss=88.99937766195083\n",
      "Current iteration=1150, the loss=76.1057996547724\n",
      "Current iteration=1151, the loss=75.48736036879738\n",
      "Current iteration=1152, the loss=59.275687446063806\n",
      "Current iteration=1153, the loss=75.67477475433694\n",
      "Current iteration=1154, the loss=75.67783485549903\n",
      "Current iteration=1155, the loss=81.13530681154275\n",
      "Current iteration=1156, the loss=79.12512502405482\n",
      "Current iteration=1157, the loss=75.27682655293863\n",
      "Current iteration=1158, the loss=69.07481099136021\n",
      "Current iteration=1159, the loss=87.37051743458207\n",
      "Current iteration=1160, the loss=66.69456916481536\n",
      "Current iteration=1161, the loss=75.80624394266755\n",
      "Current iteration=1162, the loss=78.46908343873153\n",
      "Current iteration=1163, the loss=79.33501466409399\n",
      "Current iteration=1164, the loss=79.915301517426\n",
      "Current iteration=1165, the loss=80.29651660974451\n",
      "Current iteration=1166, the loss=70.0469124415828\n",
      "Current iteration=1167, the loss=76.72653857797366\n",
      "Current iteration=1168, the loss=73.16272276369625\n",
      "Current iteration=1169, the loss=61.575811030149936\n",
      "Current iteration=1170, the loss=70.83590276769128\n",
      "Current iteration=1171, the loss=76.69305723290776\n",
      "Current iteration=1172, the loss=81.16984224468752\n",
      "Current iteration=1173, the loss=73.96209860109681\n",
      "Current iteration=1174, the loss=73.52054486231864\n",
      "Current iteration=1175, the loss=75.72551545884787\n",
      "Current iteration=1176, the loss=84.51893252444813\n",
      "Current iteration=1177, the loss=81.59028222331675\n",
      "Current iteration=1178, the loss=82.4481041449844\n",
      "Current iteration=1179, the loss=74.66773017191548\n",
      "Current iteration=1180, the loss=70.43357518222274\n",
      "Current iteration=1181, the loss=71.50789871222553\n",
      "Current iteration=1182, the loss=68.60455241086663\n",
      "Current iteration=1183, the loss=78.77808067491813\n",
      "Current iteration=1184, the loss=68.37101076494167\n",
      "Current iteration=1185, the loss=79.53848353333926\n",
      "Current iteration=1186, the loss=87.292992952433\n",
      "Current iteration=1187, the loss=95.11230857979919\n",
      "Current iteration=1188, the loss=69.99983152201415\n",
      "Current iteration=1189, the loss=80.1502755673929\n",
      "Current iteration=1190, the loss=74.21679881025577\n",
      "Current iteration=1191, the loss=87.07046168864488\n",
      "Current iteration=1192, the loss=75.75239669866396\n",
      "Current iteration=1193, the loss=73.61677966318801\n",
      "Current iteration=1194, the loss=79.59880352769633\n",
      "Current iteration=1195, the loss=74.70832123271464\n",
      "Current iteration=1196, the loss=73.49253957967176\n",
      "Current iteration=1197, the loss=75.38725721106997\n",
      "Current iteration=1198, the loss=82.58054577636491\n",
      "Current iteration=1199, the loss=60.42587284053057\n",
      "Current iteration=1200, the loss=70.05323079604409\n",
      "Current iteration=1201, the loss=80.81725398241872\n",
      "Current iteration=1202, the loss=80.33356393920417\n",
      "Current iteration=1203, the loss=74.50825220689752\n",
      "Current iteration=1204, the loss=78.45980568622429\n",
      "Current iteration=1205, the loss=74.25531218793438\n",
      "Current iteration=1206, the loss=80.09759999177463\n",
      "Current iteration=1207, the loss=76.94509643539119\n",
      "Current iteration=1208, the loss=90.43139351295567\n",
      "Current iteration=1209, the loss=70.63977780594551\n",
      "Current iteration=1210, the loss=67.29135055974966\n",
      "Current iteration=1211, the loss=84.54334889477278\n",
      "Current iteration=1212, the loss=76.28431343511656\n",
      "Current iteration=1213, the loss=70.8954051668886\n",
      "Current iteration=1214, the loss=66.42746314739009\n",
      "Current iteration=1215, the loss=60.0533534645233\n",
      "Current iteration=1216, the loss=75.26273754004288\n",
      "Current iteration=1217, the loss=74.35865347567145\n",
      "Current iteration=1218, the loss=70.2109391036056\n",
      "Current iteration=1219, the loss=69.03404539498747\n",
      "Current iteration=1220, the loss=67.3581954977914\n",
      "Current iteration=1221, the loss=78.30229556260922\n",
      "Current iteration=1222, the loss=80.39754569081012\n",
      "Current iteration=1223, the loss=80.99477820705232\n",
      "Current iteration=1224, the loss=78.40446336070802\n",
      "Current iteration=1225, the loss=77.61623242078835\n",
      "Current iteration=1226, the loss=73.67881431410356\n",
      "Current iteration=1227, the loss=75.62131794988488\n",
      "Current iteration=1228, the loss=78.91478229278329\n",
      "Current iteration=1229, the loss=77.09356571906913\n",
      "Current iteration=1230, the loss=72.68971440739253\n",
      "Current iteration=1231, the loss=75.56479121563596\n",
      "Current iteration=1232, the loss=83.4408541656828\n",
      "Current iteration=1233, the loss=80.8629661134807\n",
      "Current iteration=1234, the loss=84.92200137337883\n",
      "Current iteration=1235, the loss=71.90497631830584\n",
      "Current iteration=1236, the loss=67.66928931741933\n",
      "Current iteration=1237, the loss=62.27201056455975\n",
      "Current iteration=1238, the loss=103.49659633276161\n",
      "Current iteration=1239, the loss=79.20796963043873\n",
      "Current iteration=1240, the loss=73.93604802773028\n",
      "Current iteration=1241, the loss=60.36633535175406\n",
      "Current iteration=1242, the loss=85.27537833816581\n",
      "Current iteration=1243, the loss=77.70684036454921\n",
      "Current iteration=1244, the loss=75.25258489610155\n",
      "Current iteration=1245, the loss=71.09052740815048\n",
      "Current iteration=1246, the loss=69.29146393210938\n",
      "Current iteration=1247, the loss=77.53632715323664\n",
      "Current iteration=1248, the loss=88.37580442574307\n",
      "Current iteration=1249, the loss=77.74533699181733\n",
      "Current iteration=1250, the loss=66.07392966179304\n",
      "Current iteration=1251, the loss=79.52201611821792\n",
      "Current iteration=1252, the loss=73.64943676167411\n",
      "Current iteration=1253, the loss=78.85482384244439\n",
      "Current iteration=1254, the loss=76.54331067971464\n",
      "Current iteration=1255, the loss=65.52995390792991\n",
      "Current iteration=1256, the loss=69.03526502798518\n",
      "Current iteration=1257, the loss=75.3687820033685\n",
      "Current iteration=1258, the loss=79.91519807186833\n",
      "Current iteration=1259, the loss=73.33292887561123\n",
      "Current iteration=1260, the loss=71.22827331410673\n",
      "Current iteration=1261, the loss=74.72183650447126\n",
      "Current iteration=1262, the loss=75.06129575521625\n",
      "Current iteration=1263, the loss=80.52013072846782\n",
      "Current iteration=1264, the loss=81.27112698488477\n",
      "Current iteration=1265, the loss=67.1804759815611\n",
      "Current iteration=1266, the loss=87.38534752547017\n",
      "Current iteration=1267, the loss=64.91080675703091\n",
      "Current iteration=1268, the loss=69.10307092840603\n",
      "Current iteration=1269, the loss=68.32397249493789\n",
      "Current iteration=1270, the loss=63.91971322635319\n",
      "Current iteration=1271, the loss=67.85216449474605\n",
      "Current iteration=1272, the loss=70.17784778292427\n",
      "Current iteration=1273, the loss=73.98140154002145\n",
      "Current iteration=1274, the loss=77.6907824839823\n",
      "Current iteration=1275, the loss=66.71268716031929\n",
      "Current iteration=1276, the loss=79.84229399895219\n",
      "Current iteration=1277, the loss=74.01591125449954\n",
      "Current iteration=1278, the loss=69.7747624068318\n",
      "Current iteration=1279, the loss=76.37936010258613\n",
      "Current iteration=1280, the loss=73.86348152686469\n",
      "Current iteration=1281, the loss=82.67355867698299\n",
      "Current iteration=1282, the loss=63.0652343218805\n",
      "Current iteration=1283, the loss=74.37162477477352\n",
      "Current iteration=1284, the loss=77.32920287776989\n",
      "Current iteration=1285, the loss=70.13324008175464\n",
      "Current iteration=1286, the loss=66.31738261116017\n",
      "Current iteration=1287, the loss=79.1568483339859\n",
      "Current iteration=1288, the loss=78.08138983940569\n",
      "Current iteration=1289, the loss=84.8327409665757\n",
      "Current iteration=1290, the loss=72.90908628200486\n",
      "Current iteration=1291, the loss=68.39083804133327\n",
      "Current iteration=1292, the loss=75.04370817953844\n",
      "Current iteration=1293, the loss=67.5923228823458\n",
      "Current iteration=1294, the loss=73.46399632164548\n",
      "Current iteration=1295, the loss=80.65688952504533\n",
      "Current iteration=1296, the loss=82.26941124744928\n",
      "Current iteration=1297, the loss=72.38391389270475\n",
      "Current iteration=1298, the loss=77.81517338668493\n",
      "Current iteration=1299, the loss=64.41560008621752\n",
      "Current iteration=1300, the loss=86.39379364071866\n",
      "Current iteration=1301, the loss=74.65219836545178\n",
      "Current iteration=1302, the loss=68.25241206360506\n",
      "Current iteration=1303, the loss=75.80247920099387\n",
      "Current iteration=1304, the loss=66.6343593516878\n",
      "Current iteration=1305, the loss=70.96667931722686\n",
      "Current iteration=1306, the loss=75.07490716085869\n",
      "Current iteration=1307, the loss=69.70772193168459\n",
      "Current iteration=1308, the loss=78.17713248112358\n",
      "Current iteration=1309, the loss=73.33254903619294\n",
      "Current iteration=1310, the loss=70.41663399579932\n",
      "Current iteration=1311, the loss=62.35209673821148\n",
      "Current iteration=1312, the loss=75.2056013204297\n",
      "Current iteration=1313, the loss=66.80773676906725\n",
      "Current iteration=1314, the loss=76.58449949246685\n",
      "Current iteration=1315, the loss=83.51937699395356\n",
      "Current iteration=1316, the loss=71.6213498381006\n",
      "Current iteration=1317, the loss=71.40986620392306\n",
      "Current iteration=1318, the loss=84.16826464890067\n",
      "Current iteration=1319, the loss=69.48611854643448\n",
      "Current iteration=1320, the loss=69.0042807455562\n",
      "Current iteration=1321, the loss=70.21230728838304\n",
      "Current iteration=1322, the loss=75.85778120627833\n",
      "Current iteration=1323, the loss=72.7650096175023\n",
      "Current iteration=1324, the loss=72.74743384087006\n",
      "Current iteration=1325, the loss=69.89359546770999\n",
      "Current iteration=1326, the loss=70.77866500787925\n",
      "Current iteration=1327, the loss=75.97418131377061\n",
      "Current iteration=1328, the loss=75.84195581760159\n",
      "Current iteration=1329, the loss=66.01978005403583\n",
      "Current iteration=1330, the loss=72.17873581847323\n",
      "Current iteration=1331, the loss=80.8586970034111\n",
      "Current iteration=1332, the loss=72.15940696799312\n",
      "Current iteration=1333, the loss=64.17245200353022\n",
      "Current iteration=1334, the loss=75.47726451457585\n",
      "Current iteration=1335, the loss=69.87411209423385\n",
      "Current iteration=1336, the loss=81.535092325117\n",
      "Current iteration=1337, the loss=64.71435228364955\n",
      "Current iteration=1338, the loss=62.2344062233372\n",
      "Current iteration=1339, the loss=75.10209405062369\n",
      "Current iteration=1340, the loss=80.48348087757046\n",
      "Current iteration=1341, the loss=72.54308221943893\n",
      "Current iteration=1342, the loss=71.64502824375282\n",
      "Current iteration=1343, the loss=73.07047610483403\n",
      "Current iteration=1344, the loss=75.11566910606389\n",
      "Current iteration=1345, the loss=64.75016630641393\n",
      "Current iteration=1346, the loss=82.8740797479254\n",
      "Current iteration=1347, the loss=74.15932433650313\n",
      "Current iteration=1348, the loss=70.33394320988705\n",
      "Current iteration=1349, the loss=68.22285869649264\n",
      "Current iteration=1350, the loss=74.83272543764244\n",
      "Current iteration=1351, the loss=87.31433318546611\n",
      "Current iteration=1352, the loss=80.2828686555135\n",
      "Current iteration=1353, the loss=68.69961630676022\n",
      "Current iteration=1354, the loss=69.57926441049696\n",
      "Current iteration=1355, the loss=78.36202508700481\n",
      "Current iteration=1356, the loss=69.64528106316584\n",
      "Current iteration=1357, the loss=78.70147013572436\n",
      "Current iteration=1358, the loss=90.2727535142738\n",
      "Current iteration=1359, the loss=70.33657021144593\n",
      "Current iteration=1360, the loss=76.03886125431379\n",
      "Current iteration=1361, the loss=69.78063216771199\n",
      "Current iteration=1362, the loss=84.73694898466414\n",
      "Current iteration=1363, the loss=80.9410645250537\n",
      "Current iteration=1364, the loss=90.53641953186259\n",
      "Current iteration=1365, the loss=75.08313522556882\n",
      "Current iteration=1366, the loss=65.65528064902627\n",
      "Current iteration=1367, the loss=70.03771007358577\n",
      "Current iteration=1368, the loss=73.4467184975984\n",
      "Current iteration=1369, the loss=83.74694559913922\n",
      "Current iteration=1370, the loss=76.37807924260358\n",
      "Current iteration=1371, the loss=72.02127025715619\n",
      "Current iteration=1372, the loss=80.28004805110729\n",
      "Current iteration=1373, the loss=63.756055145589094\n",
      "Current iteration=1374, the loss=81.08641514930846\n",
      "Current iteration=1375, the loss=87.65541382230691\n",
      "Current iteration=1376, the loss=75.60020487169282\n",
      "Current iteration=1377, the loss=66.5208805917144\n",
      "Current iteration=1378, the loss=74.79733062822666\n",
      "Current iteration=1379, the loss=72.83349530523132\n",
      "Current iteration=1380, the loss=74.72218148611643\n",
      "Current iteration=1381, the loss=73.10701047098755\n",
      "Current iteration=1382, the loss=70.50408505826715\n",
      "Current iteration=1383, the loss=70.33927841480595\n",
      "Current iteration=1384, the loss=74.95544418468012\n",
      "Current iteration=1385, the loss=72.70910467179647\n",
      "Current iteration=1386, the loss=77.81871969976716\n",
      "Current iteration=1387, the loss=70.12854619420489\n",
      "Current iteration=1388, the loss=79.70860171604512\n",
      "Current iteration=1389, the loss=77.87589029977565\n",
      "Current iteration=1390, the loss=81.15004181141356\n",
      "Current iteration=1391, the loss=72.91383096897832\n",
      "Current iteration=1392, the loss=71.34605669119381\n",
      "Current iteration=1393, the loss=75.50543493853414\n",
      "Current iteration=1394, the loss=70.74053726432848\n",
      "Current iteration=1395, the loss=74.7313441240295\n",
      "Current iteration=1396, the loss=76.98492834220185\n",
      "Current iteration=1397, the loss=74.56885653402402\n",
      "Current iteration=1398, the loss=80.3782678868296\n",
      "Current iteration=1399, the loss=74.88791576441126\n",
      "Current iteration=1400, the loss=75.25359147886557\n",
      "Current iteration=1401, the loss=73.3492048278791\n",
      "Current iteration=1402, the loss=54.59302007995354\n",
      "Current iteration=1403, the loss=63.77281423199156\n",
      "Current iteration=1404, the loss=70.13116776529543\n",
      "Current iteration=1405, the loss=80.88095347563464\n",
      "Current iteration=1406, the loss=75.04298450937522\n",
      "Current iteration=1407, the loss=75.5741701771496\n",
      "Current iteration=1408, the loss=84.79534970978698\n",
      "Current iteration=1409, the loss=74.15127259620706\n",
      "Current iteration=1410, the loss=61.77940528325986\n",
      "Current iteration=1411, the loss=67.71789059708898\n",
      "Current iteration=1412, the loss=75.22141716012281\n",
      "Current iteration=1413, the loss=63.239437672655384\n",
      "Current iteration=1414, the loss=90.02282489795864\n",
      "Current iteration=1415, the loss=76.3317003288013\n",
      "Current iteration=1416, the loss=67.83943573105721\n",
      "Current iteration=1417, the loss=77.57666340982898\n",
      "Current iteration=1418, the loss=77.94240036884975\n",
      "Current iteration=1419, the loss=75.79410680837557\n",
      "Current iteration=1420, the loss=83.72867593747405\n",
      "Current iteration=1421, the loss=78.38736620438542\n",
      "Current iteration=1422, the loss=75.65935111833903\n",
      "Current iteration=1423, the loss=73.2336812660559\n",
      "Current iteration=1424, the loss=64.18038214389577\n",
      "Current iteration=1425, the loss=70.90319391708704\n",
      "Current iteration=1426, the loss=82.88049674627804\n",
      "Current iteration=1427, the loss=79.64962611688918\n",
      "Current iteration=1428, the loss=73.96692688602393\n",
      "Current iteration=1429, the loss=73.81548679025511\n",
      "Current iteration=1430, the loss=68.0752485171179\n",
      "Current iteration=1431, the loss=72.81550060043611\n",
      "Current iteration=1432, the loss=76.97603096205333\n",
      "Current iteration=1433, the loss=71.75455461223346\n",
      "Current iteration=1434, the loss=77.9995488736667\n",
      "Current iteration=1435, the loss=74.32343163682513\n",
      "Current iteration=1436, the loss=84.46022510849915\n",
      "Current iteration=1437, the loss=78.52863798284014\n",
      "Current iteration=1438, the loss=69.55373804791043\n",
      "Current iteration=1439, the loss=72.15968341323378\n",
      "Current iteration=1440, the loss=84.0726699726741\n",
      "Current iteration=1441, the loss=71.98884261246482\n",
      "Current iteration=1442, the loss=69.35694534314146\n",
      "Current iteration=1443, the loss=81.82423364794609\n",
      "Current iteration=1444, the loss=79.08793684188191\n",
      "Current iteration=1445, the loss=75.72335882102044\n",
      "Current iteration=1446, the loss=71.25987302313962\n",
      "Current iteration=1447, the loss=83.4236833075729\n",
      "Current iteration=1448, the loss=76.37618528808706\n",
      "Current iteration=1449, the loss=69.98145246918907\n",
      "Current iteration=1450, the loss=67.54199064549714\n",
      "Current iteration=1451, the loss=72.9724973312377\n",
      "Current iteration=1452, the loss=70.47665644639807\n",
      "Current iteration=1453, the loss=76.10366039338001\n",
      "Current iteration=1454, the loss=78.52418216925913\n",
      "Current iteration=1455, the loss=86.3118225966896\n",
      "Current iteration=1456, the loss=75.42806250954538\n",
      "Current iteration=1457, the loss=92.20952042320386\n",
      "Current iteration=1458, the loss=60.83471124274365\n",
      "Current iteration=1459, the loss=74.00906513438903\n",
      "Current iteration=1460, the loss=89.81712446243839\n",
      "Current iteration=1461, the loss=83.55884136638713\n",
      "Current iteration=1462, the loss=76.24395024141826\n",
      "Current iteration=1463, the loss=66.70676081674254\n",
      "Current iteration=1464, the loss=78.03291685913776\n",
      "Current iteration=1465, the loss=78.47903660745368\n",
      "Current iteration=1466, the loss=67.82975949298525\n",
      "Current iteration=1467, the loss=80.73705354916422\n",
      "Current iteration=1468, the loss=65.62331490937741\n",
      "Current iteration=1469, the loss=82.49951784497938\n",
      "Current iteration=1470, the loss=67.85239903717283\n",
      "Current iteration=1471, the loss=73.63119315132599\n",
      "Current iteration=1472, the loss=69.00250317954307\n",
      "Current iteration=1473, the loss=72.34947633547003\n",
      "Current iteration=1474, the loss=72.90738285070537\n",
      "Current iteration=1475, the loss=66.54550470165864\n",
      "Current iteration=1476, the loss=75.0618544893117\n",
      "Current iteration=1477, the loss=73.25814574300215\n",
      "Current iteration=1478, the loss=72.30596566714178\n",
      "Current iteration=1479, the loss=70.73764373546238\n",
      "Current iteration=1480, the loss=79.58898639183946\n",
      "Current iteration=1481, the loss=74.17865460830544\n",
      "Current iteration=1482, the loss=81.37874693298045\n",
      "Current iteration=1483, the loss=76.68027345941876\n",
      "Current iteration=1484, the loss=75.53366004706082\n",
      "Current iteration=1485, the loss=67.27488645851292\n",
      "Current iteration=1486, the loss=75.59601422463909\n",
      "Current iteration=1487, the loss=77.74021442101417\n",
      "Current iteration=1488, the loss=75.6861721614361\n",
      "Current iteration=1489, the loss=82.13349125196223\n",
      "Current iteration=1490, the loss=74.86631876535311\n",
      "Current iteration=1491, the loss=61.40110331411337\n",
      "Current iteration=1492, the loss=74.42871770870767\n",
      "Current iteration=1493, the loss=83.62215171384118\n",
      "Current iteration=1494, the loss=73.48626288545938\n",
      "Current iteration=1495, the loss=65.06089113604173\n",
      "Current iteration=1496, the loss=69.68472727509806\n",
      "Current iteration=1497, the loss=77.10580626806994\n",
      "Current iteration=1498, the loss=75.60261208330189\n",
      "Current iteration=1499, the loss=75.49577549799355\n",
      "Current iteration=1500, the loss=81.56048734914994\n",
      "Current iteration=1501, the loss=73.46805743436192\n",
      "Current iteration=1502, the loss=69.9661063184103\n",
      "Current iteration=1503, the loss=84.16231183133911\n",
      "Current iteration=1504, the loss=73.3760185221113\n",
      "Current iteration=1505, the loss=72.73824857124796\n",
      "Current iteration=1506, the loss=74.34581257661856\n",
      "Current iteration=1507, the loss=74.28085400386053\n",
      "Current iteration=1508, the loss=71.93889003707929\n",
      "Current iteration=1509, the loss=82.66873179983406\n",
      "Current iteration=1510, the loss=66.06820338812344\n",
      "Current iteration=1511, the loss=66.32334591618907\n",
      "Current iteration=1512, the loss=67.4150111778778\n",
      "Current iteration=1513, the loss=75.8179582894704\n",
      "Current iteration=1514, the loss=69.56510359861048\n",
      "Current iteration=1515, the loss=61.15808299421829\n",
      "Current iteration=1516, the loss=77.07608878557903\n",
      "Current iteration=1517, the loss=80.27059058525255\n",
      "Current iteration=1518, the loss=83.93794976359791\n",
      "Current iteration=1519, the loss=71.891244865668\n",
      "Current iteration=1520, the loss=72.03323445581071\n",
      "Current iteration=1521, the loss=73.95436830021949\n",
      "Current iteration=1522, the loss=76.74256938941903\n",
      "Current iteration=1523, the loss=76.3957963161371\n",
      "Current iteration=1524, the loss=73.5669462715984\n",
      "Current iteration=1525, the loss=72.00301123720132\n",
      "Current iteration=1526, the loss=67.19417838128096\n",
      "Current iteration=1527, the loss=76.93719755429453\n",
      "Current iteration=1528, the loss=80.61865981999249\n",
      "Current iteration=1529, the loss=89.2117750381014\n",
      "Current iteration=1530, the loss=72.74249296905094\n",
      "Current iteration=1531, the loss=77.3685116396611\n",
      "Current iteration=1532, the loss=76.89178630594981\n",
      "Current iteration=1533, the loss=75.69942906677119\n",
      "Current iteration=1534, the loss=71.79556299966936\n",
      "Current iteration=1535, the loss=72.19538883187461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=1536, the loss=70.67884770778258\n",
      "Current iteration=1537, the loss=74.14504265419637\n",
      "Current iteration=1538, the loss=89.53835108216015\n",
      "Current iteration=1539, the loss=72.35721540930021\n",
      "Current iteration=1540, the loss=73.34278147540907\n",
      "Current iteration=1541, the loss=78.35076215162971\n",
      "Current iteration=1542, the loss=74.57375761367092\n",
      "Current iteration=1543, the loss=69.49957172313003\n",
      "Current iteration=1544, the loss=77.96967938769856\n",
      "Current iteration=1545, the loss=77.58773823540783\n",
      "Current iteration=1546, the loss=65.4346153325553\n",
      "Current iteration=1547, the loss=76.06271719910987\n",
      "Current iteration=1548, the loss=81.58881856741587\n",
      "Current iteration=1549, the loss=80.77789731595782\n",
      "Current iteration=1550, the loss=79.75792277155325\n",
      "Current iteration=1551, the loss=67.74513044854804\n",
      "Current iteration=1552, the loss=64.77466554031196\n",
      "Current iteration=1553, the loss=81.04150642372764\n",
      "Current iteration=1554, the loss=72.99868879335756\n",
      "Current iteration=1555, the loss=84.01172205997005\n",
      "Current iteration=1556, the loss=84.88820168146452\n",
      "Current iteration=1557, the loss=76.29635463957122\n",
      "Current iteration=1558, the loss=78.50086239511387\n",
      "Current iteration=1559, the loss=79.41893248382775\n",
      "Current iteration=1560, the loss=65.51108738541029\n",
      "Current iteration=1561, the loss=82.72624843899396\n",
      "Current iteration=1562, the loss=76.52534422094412\n",
      "Current iteration=1563, the loss=83.7234733551368\n",
      "Current iteration=1564, the loss=66.15100319156387\n",
      "Current iteration=1565, the loss=75.0608924651812\n",
      "Current iteration=1566, the loss=72.68859714712539\n",
      "Current iteration=1567, the loss=79.2772453432811\n",
      "Current iteration=1568, the loss=82.07827149265569\n",
      "Current iteration=1569, the loss=71.86273955796429\n",
      "Current iteration=1570, the loss=81.21248043459781\n",
      "Current iteration=1571, the loss=76.02914459594321\n",
      "Current iteration=1572, the loss=72.47677539174411\n",
      "Current iteration=1573, the loss=73.78497914449693\n",
      "Current iteration=1574, the loss=67.206284999389\n",
      "Current iteration=1575, the loss=78.58581696100497\n",
      "Current iteration=1576, the loss=67.72973525883644\n",
      "Current iteration=1577, the loss=73.59576436903255\n",
      "Current iteration=1578, the loss=76.28198241986121\n",
      "Current iteration=1579, the loss=67.48537679099371\n",
      "Current iteration=1580, the loss=71.86761787419843\n",
      "Current iteration=1581, the loss=74.21074150655366\n",
      "Current iteration=1582, the loss=73.18430420096544\n",
      "Current iteration=1583, the loss=71.78115924299948\n",
      "Current iteration=1584, the loss=77.37617796438666\n",
      "Current iteration=1585, the loss=71.56515396011184\n",
      "Current iteration=1586, the loss=77.39591692834526\n",
      "Current iteration=1587, the loss=72.27382675492674\n",
      "Current iteration=1588, the loss=78.38060948044425\n",
      "Current iteration=1589, the loss=77.72344701363764\n",
      "Current iteration=1590, the loss=70.09797350228696\n",
      "Current iteration=1591, the loss=74.47598273871945\n",
      "Current iteration=1592, the loss=69.61949695203725\n",
      "Current iteration=1593, the loss=74.84451100974223\n",
      "Current iteration=1594, the loss=70.10730569607733\n",
      "Current iteration=1595, the loss=71.32279552456033\n",
      "Current iteration=1596, the loss=77.83577445126588\n",
      "Current iteration=1597, the loss=82.3127341484244\n",
      "Current iteration=1598, the loss=74.69996462974869\n",
      "Current iteration=1599, the loss=73.40565721785879\n",
      "Current iteration=1600, the loss=79.4514623582993\n",
      "Current iteration=1601, the loss=71.79192530912454\n",
      "Current iteration=1602, the loss=81.6070241800682\n",
      "Current iteration=1603, the loss=78.88857567479678\n",
      "Current iteration=1604, the loss=57.33617497965382\n",
      "Current iteration=1605, the loss=70.12207172591673\n",
      "Current iteration=1606, the loss=78.21422491700517\n",
      "Current iteration=1607, the loss=82.61518259326166\n",
      "Current iteration=1608, the loss=74.09094734892152\n",
      "Current iteration=1609, the loss=79.39016263901999\n",
      "Current iteration=1610, the loss=84.12764671008617\n",
      "Current iteration=1611, the loss=78.16777847695998\n",
      "Current iteration=1612, the loss=77.72331255690429\n",
      "Current iteration=1613, the loss=71.77264676074046\n",
      "Current iteration=1614, the loss=72.11832966217193\n",
      "Current iteration=1615, the loss=96.62571784046875\n",
      "Current iteration=1616, the loss=65.33805127625759\n",
      "Current iteration=1617, the loss=81.52459483667366\n",
      "Current iteration=1618, the loss=66.1651697979868\n",
      "Current iteration=1619, the loss=64.31715913938811\n",
      "Current iteration=1620, the loss=75.9231223365208\n",
      "Current iteration=1621, the loss=66.37874489427037\n",
      "Current iteration=1622, the loss=86.70609695036445\n",
      "Current iteration=1623, the loss=73.65984958509303\n",
      "Current iteration=1624, the loss=79.050124297155\n",
      "Current iteration=1625, the loss=76.43421832908038\n",
      "Current iteration=1626, the loss=77.4044228152302\n",
      "Current iteration=1627, the loss=78.78380264484295\n",
      "Current iteration=1628, the loss=73.81724512308938\n",
      "Current iteration=1629, the loss=70.6091087275698\n",
      "Current iteration=1630, the loss=70.30223296725822\n",
      "Current iteration=1631, the loss=85.09941028524862\n",
      "Current iteration=1632, the loss=72.59656610244684\n",
      "Current iteration=1633, the loss=72.64725925078906\n",
      "Current iteration=1634, the loss=77.93943841555267\n",
      "Current iteration=1635, the loss=75.64158149493008\n",
      "Current iteration=1636, the loss=59.21282661828901\n",
      "Current iteration=1637, the loss=71.33531483468911\n",
      "Current iteration=1638, the loss=69.8107902782033\n",
      "Current iteration=1639, the loss=86.38403095591596\n",
      "Current iteration=1640, the loss=73.86579761464486\n",
      "Current iteration=1641, the loss=74.16843742214982\n",
      "Current iteration=1642, the loss=80.7606506515541\n",
      "Current iteration=1643, the loss=75.76596136123005\n",
      "Current iteration=1644, the loss=75.99518926942088\n",
      "Current iteration=1645, the loss=63.954711217586144\n",
      "Current iteration=1646, the loss=71.51260625261804\n",
      "Current iteration=1647, the loss=82.54285048409017\n",
      "Current iteration=1648, the loss=74.555905341402\n",
      "Current iteration=1649, the loss=76.66848683427293\n",
      "Current iteration=1650, the loss=73.23601923063205\n",
      "Current iteration=1651, the loss=87.78770131361057\n",
      "Current iteration=1652, the loss=76.0350974123145\n",
      "Current iteration=1653, the loss=70.30680428775457\n",
      "Current iteration=1654, the loss=68.93524490396626\n",
      "Current iteration=1655, the loss=88.3825146506044\n",
      "Current iteration=1656, the loss=71.83124203100301\n",
      "Current iteration=1657, the loss=66.82294145057202\n",
      "Current iteration=1658, the loss=82.3224299812658\n",
      "Current iteration=1659, the loss=80.7362709906281\n",
      "Current iteration=1660, the loss=70.89163035866986\n",
      "Current iteration=1661, the loss=83.63491308973704\n",
      "Current iteration=1662, the loss=77.60835266927091\n",
      "Current iteration=1663, the loss=79.53396520593397\n",
      "Current iteration=1664, the loss=77.48459283308347\n",
      "Current iteration=1665, the loss=82.32476961714706\n",
      "Current iteration=1666, the loss=81.47244405424429\n",
      "Current iteration=1667, the loss=57.159910570631915\n",
      "Current iteration=1668, the loss=0.0\n",
      "Current iteration=1669, the loss=0.0\n"
     ]
    }
   ],
   "source": [
    "l, w, data = imp.stochastic_logistic_regression(y_train, std_x_train, max_iter=25000, batch_size=150, threshold=10**(-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1668"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f5859ce0cf8>]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhUAIIQQAFQIhhBANKgRCCCEAgP8HRlGtv3ti1ccAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f589b253b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw least squares (score: ?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(w, loss) = imp.least_squares(y_train, x_train)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Std least squares (score: 0.73)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(w, loss) = imp.least_squares(y_train, col_std_x_train)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(w, loss) = imp.least_squares(y_train, std_x_train)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_1_column(mat):\n",
    "    return np.hstack((np.ones((mat.shape[0], 1)), mat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_loss(y, tx, w):                                                        \n",
    "    \"\"\"Computes loss using log-likelihood\"\"\"                                \n",
    "    txw = tx @ w                                                            \n",
    "    return np.sum(np.log(1 + np.exp(txw)) - y @ txw.T)          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp.cross_validation_v2(y_train, col_std_x_train, 2, f, degree=1, compute_loss=compute_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvalues reduction (score: 0.62)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.matrix([[1,2],[3,4]])\n",
    "np.tile(a, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "std_x_train = standardize(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u, s, v = np.linalg.svd(std_x_train, full_matrices=False)\n",
    "print('u shape:', u.shape)\n",
    "print('s shape:', s.shape)\n",
    "print('v shape:', v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(s)\n",
    "plt.yscale('log')\n",
    "plt.title('log')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shortened_x_train = u[:, :23] @ np.diag(s[:23]) @ v[:23,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortened_x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(w, loss) = imp.least_squares(y_train, x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = helper.predict_labels(w, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of boson:', np.count_nonzero(y_pred+1))\n",
    "print('Number of other:', np.count_nonzero(y_pred-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "helper.create_csv_submission(ids_test, y_pred, 'shortened_eigenvalues_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique_values_per_column_count = [len(set(col)) for col in train_data.T]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique_values_per_column_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def y_map(y):\n",
    "    if y == -1:\n",
    "        return 0\n",
    "    else:\n",
    "        return 0.2\n",
    "\n",
    "y_train_mapped = np.vectorize(y_map)(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "square_train_data = std_x_train.T @ std_x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w, v = np.linalg.eigh(square_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(w.shape[0])\n",
    "print(v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def keep_variance(percentage, vec):\n",
    "    r = list(range(1, w.shape[0] + 1))\n",
    "    total = np.sum(vec)\n",
    "    sums = list(map(lambda i: np.sum(vec[-i:]), r))\n",
    "    ratio = sums / total\n",
    "    return np.argmin(abs(ratio - percentage)) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_keeper = keep_variance(0.9, w)\n",
    "print(index_keeper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(w)\n",
    "plt.yscale('log')\n",
    "plt.title('log')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filtered_v = v[:,-index_keeper:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "project_x_train = std_x_train @ filtered_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tupled_boson = np.array(list(zip(*filter(lambda pair: pair[1] == -1, zip(project_x_train.tolist(), y_train))))[0])\n",
    "tupled_other = np.array(list(zip(*filter(lambda pair: pair[1] == 1, zip(project_x_train.tolist(), y_train))))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tupled_boson.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(tupled_boson[1000:2000, :1], tupled_boson[1000:2000, 1:], 'bo')\n",
    "plt.plot(tupled_other[1000:2000, :1], tupled_other[1000:2000, 1:], 'ro')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(weight, loss) = imp.least_squares(y_train, project_x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = helper.predict_labels(weight, std_x_test @ filtered_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of boson:', np.count_nonzero(y_pred+1))\n",
    "print('Number of other:', np.count_nonzero(y_pred-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Polynomial feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.array(range(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def polynomial_enhancement(x, deg):\n",
    "    stacked_x = np.tile(x, deg+1)\n",
    "    power_vec = np.repeat(np.array(range(deg+1)), x.shape[1])\n",
    "    return stacked_x ** power_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enhanced_col_std_x_train = polynomial_enhancement(std_x_train, 9)\n",
    "enhanced_col_std_x_test = polynomial_enhancement(std_x_test, 9)\n",
    "enhanced_col_std_x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pseudo_least_squares(y, x):\n",
    "    U, S, V = np.linalg.svd(x, full_matrices=False)\n",
    "    w = V.T @ np.diag(1/S) @ U.T @ y\n",
    "    loss = imp.mse(y, x, w)\n",
    "    return (w, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(w, loss) = pseudo_least_squares(y_train, enhanced_col_std_x_train)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = helper.predict_labels(w, enhanced_col_std_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "helper.create_csv_submission(ids_test, y_pred, 'basic_poly_enhancement_9.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "acc, loss_train, loss_test, w = imp.cross_validation_v2(y_train, std_x_train, 10, imp.pseudo_least_squares, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enhanced_col_std_x_test = imp.polynomial_enhancement(std_x_test, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = helper.predict_labels(w, enhanced_col_std_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "helper.create_csv_submission(ids_test, y_pred, '0_8_accuracy_poly_5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = []\n",
    "\n",
    "test.append(1)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "acc, loss_train, loss_test, w = imp.cross_validation_v2(y_train, std_x_train, 10, imp.pseudo_least_squares, 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
