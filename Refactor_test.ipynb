{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import scripts.proj1_helpers as helper\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "#import implementations as imp\n",
    "import cross_validation as cv\n",
    "#from preprocessor import Preprocessor\n",
    "from model_ensembler import ModelEnsembler\n",
    "import preprocessing\n",
    "\n",
    "import run\n",
    "from least_squares import LeastSquares\n",
    "from logistic import LogisticRegression\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def global_preprocess(x_train, x_test, deg=1):\n",
    "    x_train_end = x_train.shape[0]\n",
    "    \n",
    "    x_stacked = np.vstack((x_train, x_test))\n",
    "    \n",
    "    x = x_stacked.copy()\n",
    "    stds = np.std(x, axis=0)\n",
    "    deleted_cols_ids = np.where(stds == 0)\n",
    "    x = np.delete(x, deleted_cols_ids, axis=1)\n",
    "    run.mean_spec(x)\n",
    "    x = run.standardize(x)\n",
    "    #x = run.polynomial_enhancement(x, deg)\n",
    "    return x[:x_train_end], x[x_train_end:]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_train, x_train, ids_train = helper.load_csv_data('train.csv')\n",
    "y_test, x_test, ids_test = helper.load_csv_data('test.csv')\n",
    "y_train[y_train < 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train, x_train, ids_train = helper.load_csv_data('train.csv')\n",
    "y_test, x_test, ids_test = helper.load_csv_data('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess option 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_test = global_preprocess(x_train, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_test = preprocessing.preprocess(x_train, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_1 = LogisticRegression(degree=5, gamma=0.1)\n",
    "lr_2 = LogisticRegression(degree=3, gamma=0.1)\n",
    "lr_meta = LogisticRegression(degree=1, gamma=0.1)\n",
    "\n",
    "model_ensembler = ModelEnsembler([lr_1, lr_2], lr_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 / 5\n",
      "0.81856\n",
      "Step 2 / 5\n",
      "0.81724\n",
      "Step 3 / 5\n",
      "0.81776\n",
      "Step 4 / 5\n",
      "0.81712\n",
      "Step 5 / 5\n",
      "0.81704\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.81855999999999995,\n",
       " 0.81723999999999997,\n",
       " 0.81776000000000004,\n",
       " 0.81711999999999996,\n",
       " 0.81703999999999999]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = cv.cross_validation(y_train, x_train, 5, lr_1, seed=1)\n",
    "accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_ensembler.train(y_train, x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = model_ensembler.predict_labels(x_test)\n",
    "preds[preds == 0] = -1\n",
    "helper.create_csv_submission(ids_test, preds, 'model_ensembling_global_preprocess_logistic_regression_4_3_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "175715"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[preds > 0].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81210799999999994"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(accuracy) / len(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81210799999999994"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(accuracy) / len(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess option 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic cat cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_outliers(y, x, threshold=0.1):\n",
    "    x = x.copy()\n",
    "    stds = np.std(x, axis=0)\n",
    "    means = np.mean(x, axis=0)\n",
    "    centered = np.absolute(x - means)  \n",
    "    variance_ratio = centered / stds\n",
    "    rows_to_keep = np.any(variance_ratio < threshold, axis=1).reshape(y.shape)\n",
    "    return y[rows_to_keep], x[rows_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cat number: 0 with: 99913 elements\n",
      "Step 1 / 5\n",
      "0.838955059554\n",
      "Step 2 / 5\n",
      "0.836002402162\n",
      "Step 3 / 5\n",
      "0.838154338905\n",
      "Step 4 / 5\n",
      "0.842207987188\n",
      "Step 5 / 5\n",
      "0.847112401161\n",
      "\n",
      "Cat number: 1 with: 77544 elements\n",
      "Step 1 / 5\n",
      "0.801134898117\n",
      "Step 2 / 5\n",
      "0.803004900696\n",
      "Step 3 / 5\n",
      "0.804359040495\n",
      "Step 4 / 5\n",
      "0.803649729172\n",
      "Step 5 / 5\n",
      "0.803907660562\n",
      "\n",
      "Cat number: 2 with: 50379 elements\n",
      "Step 1 / 5\n",
      "0.832853598015\n",
      "Step 2 / 5\n",
      "0.828089330025\n",
      "Step 3 / 5\n",
      "0.839602977667\n",
      "Step 4 / 5\n",
      "0.837717121588\n",
      "Step 5 / 5\n",
      "0.827791563275\n",
      "\n",
      "Cat number: 3 with: 22164 elements\n",
      "Step 1 / 5\n",
      "0.824007220217\n",
      "Step 2 / 5\n",
      "0.840703971119\n",
      "Step 3 / 5\n",
      "0.827842960289\n",
      "Step 4 / 5\n",
      "0.825135379061\n",
      "Step 5 / 5\n",
      "0.834837545126\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logistic_regression_0 = LogisticRegression(degree=3, gamma=0.1)\n",
    "remove_outliers_0 = lambda y, x: remove_outliers(y, x, 0.05)\n",
    "\n",
    "logistic_regression_1 = LogisticRegression(degree=6, gamma=0.1)\n",
    "remove_outliers_1 = lambda y, x: remove_outliers(y, x, 0.2)\n",
    "\n",
    "logistic_regression_2 = LogisticRegression(degree=6, gamma=0.1)\n",
    "remove_outliers_2 = lambda y, x: remove_outliers(y, x, 0.1)\n",
    "\n",
    "logistic_regression_3 = LogisticRegression(degree=6, gamma=0.1)\n",
    "remove_outliers_3 = lambda y, x: remove_outliers(y, x, 0.1)\n",
    "\n",
    "\n",
    "i = 0\n",
    "models = [logistic_regression_0, logistic_regression_1, logistic_regression_2, logistic_regression_3]\n",
    "outliers_remover = [remove_outliers_0, remove_outliers_1, remove_outliers_2, remove_outliers_3]\n",
    "#models = [logistic_regression_0, model_ensembler_1, model_ensembler_2, model_ensembler_3]\n",
    "accuracies = []\n",
    "\n",
    "for cat_data in run.category_iter(y_train, x_train, 22, x_test):\n",
    "    #if i != 3:\n",
    "     #   i = i + 1\n",
    "      #  continue\n",
    "    y_train_cat, x_train_cat, x_test_cat, cat_indicies_te = cat_data\n",
    "    x_train_cat, x_test_cat = preprocessing.preprocess(x_train_cat, x_test_cat)\n",
    "    #y_train_cat, x_train_cat = outliers_remover[i](y_train_cat, x_train_cat)\n",
    "    #print(y_train_test.shape)\n",
    "    print('Cat number:', i, 'with:', y_train_cat.size, 'elements')\n",
    "    \n",
    "    accuracies.append(np.array(cv.cross_validation(y_train_cat, x_train_cat, 5, models[i])))\n",
    "    i = i + 1\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82685350421980708"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies = np.ravel(np.array(accuracies))\n",
    "np.sum(accuracies) / accuracies.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82686525456742788"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies = np.ravel(np.array(accuracies))\n",
    "np.sum(accuracies) / accuracies.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82942238267148016"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies = np.ravel(np.array(accuracies))\n",
    "np.sum(accuracies) / accuracies.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic cat train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cat number: 0 with: 99913 elements\n",
      "\n",
      "Cat number: 1 with: 77544 elements\n",
      "\n",
      "Cat number: 2 with: 50379 elements\n",
      "\n",
      "Cat number: 3 with: 22164 elements\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logistic_regression_0 = LogisticRegression(degree=3, gamma=0.1)\n",
    "logistic_regression_1 = LogisticRegression(degree=6, gamma=0.1)\n",
    "logistic_regression_2 = LogisticRegression(degree=6, gamma=0.1)\n",
    "logistic_regression_3 = LogisticRegression(degree=6, gamma=0.1)\n",
    "\n",
    "i = 0\n",
    "models = [logistic_regression_0, logistic_regression_1, logistic_regression_2, logistic_regression_3]\n",
    "accuracies = []\n",
    "predictions = np.zeros((y_test.shape[0], 1))\n",
    "\n",
    "for cat_data in run.category_iter(y_train, x_train, 22, x_test):\n",
    "    #if i != 3:\n",
    "        #i = i + 1\n",
    "        #continue\n",
    "    y_train_cat, x_train_cat, x_test_cat, cat_indices_te = cat_data\n",
    "    x_train_cat, x_test_cat = preprocessing.preprocess(x_train_cat, x_test_cat)\n",
    "    print('Cat number:', i, 'with:', y_train_cat.size, 'elements')\n",
    "    \n",
    "    models[i].train(y_train_cat, x_train_cat)\n",
    "    labels = models[i].predict_labels(x_test_cat)\n",
    "    labels[labels == 0] = -1\n",
    "    \n",
    "    predictions[cat_indices_te] = labels\n",
    "    i = i + 1\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "helper.create_csv_submission(ids_test, predictions, 'logistic_cat_3_6_6_6_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least squares cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cat number: 0 with: 99913 elements\n",
      "Step 1 / 5\n",
      "0.14833350015\n",
      "Step 2 / 5\n",
      "0.149684716245\n",
      "Step 3 / 5\n",
      "0.147682914623\n",
      "Step 4 / 5\n",
      "0.149534581123\n",
      "Step 5 / 5\n",
      "0.152737463717\n",
      "\n",
      "Cat number: 1 with: 77544 elements\n",
      "Step 1 / 5\n",
      "0.243229301006\n",
      "Step 2 / 5\n",
      "0.242262058292\n",
      "Step 3 / 5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-160-bf7f7c5ad626>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cat number:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'with:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_cat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'elements'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0maccuracies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/EPFL/Master 1/Fall/MachineLearning-Project1/cross_validation.py\u001b[0m in \u001b[0;36mcross_validation\u001b[0;34m(y, x, k_fold, model, seed, compute_loss)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_fold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Step'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_fold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mtmp_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_validation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;31m# loss_tr.append(tmp_loss_tr)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;31m# loss_te.append(tmp_loss_te)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/EPFL/Master 1/Fall/MachineLearning-Project1/cross_validation.py\u001b[0m in \u001b[0;36mcross_validation_step\u001b[0;34m(k)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mtrain_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# tx = polynomial_enhancement(x[train_indices], degree)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mpredicted_raw_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/EPFL/Master 1/Fall/MachineLearning-Project1/least_squares.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, y, x)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;34m\"\"\"Trains the model on the provided x,y data\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolynomial_enhancement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdegree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/EPFL/Master 1/Fall/MachineLearning-Project1/run.py\u001b[0m in \u001b[0;36mpolynomial_enhancement\u001b[0;34m(x, deg)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mstacked_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mpower_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeg\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstacked_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacked_x\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mpower_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "least_squares_0 = LeastSquares(degree=7)\n",
    "least_squares_1 = LeastSquares(degree=12)\n",
    "least_squares_2 = LeastSquares(degree=11)\n",
    "least_squares_3 = LeastSquares(degree=11)\n",
    "\n",
    "i = 0\n",
    "models = [least_squares_0, least_squares_1, least_squares_2, least_squares_3]\n",
    "accuracies = []\n",
    "\n",
    "for cat_data in run.category_iter(y_train, x_train, 22, x_test):\n",
    "    #if i != 3:\n",
    "        #i = i + 1\n",
    "        #continue\n",
    "    y_train_cat, x_train_cat, x_test_cat, cat_indicies_te = cat_data\n",
    "    x_train_cat, x_test_cat = preprocessing.preprocess(x_train_cat, x_test_cat)\n",
    "    print('Cat number:', i, 'with:', y_train_cat.size, 'elements')\n",
    "    \n",
    "    accuracies.append(cv.cross_validation(y_train_cat, x_train_cat, 5, models[i]))\n",
    "    i = i + 1\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82526539088158013"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies = np.ravel(np.array(accuracies))\n",
    "np.sum(accuracies) / accuracies.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "least_square_model_1 = LeastSquares(degree=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy = cv.cross_validation(y_train, x_train, 5, least_square_model_1, seed=1, compute_loss=imp.rmse)\n",
    "accuracy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Garbage (kept just in case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_train(x_tr, deg=4):\n",
    "    x = x_tr.copy()\n",
    "    x = run.polynomial_enhancement(x, deg)\n",
    "    return x, None\n",
    "\n",
    "def preprocess_test(x_te, dependency, deg=4): \n",
    "    x = x_te.copy()\n",
    "    x = run.polynomial_enhancement(x, deg)\n",
    "    return x\n",
    "\n",
    "preprocess_train_model_1 = lambda x_tr: preprocess_train(x_tr, deg=4)\n",
    "preprocess_test_model_1 = lambda x_te, dependency: preprocess_test(x_te, dependency, deg=4)    \n",
    "\n",
    "def preprocess_train_meta_model(x_tr, deg=4):\n",
    "    x = x_tr.copy()\n",
    "    stacked_x = np.tile(x, deg)\n",
    "    power_vec = np.repeat(np.array(range(1, deg + 1)), x.shape[1])\n",
    "    return stacked_x ** power_vec, None\n",
    "    \n",
    "def preprocess_test_meta_model(x_te, dependency, deg=4):\n",
    "    x = x_te.copy()\n",
    "    stacked_x = np.tile(x, deg)\n",
    "    power_vec = np.repeat(np.array(range(1, deg + 1)), x.shape[1])\n",
    "    return stacked_x ** power_vec\n",
    "    \n",
    "\n",
    "#preprocess_train_meta_model = lambda x_tr: run.polynomial_enhancement(x_tr, 4), None\n",
    "#preprocess_test_meta_model = lambda x_te: run.polynomial_enhancement(x_te, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "least_square_model_1 = LeastSquares(degree=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preprocessor_1 = Preprocessor(preprocess_train, preprocess_test)\n",
    "preprocessor_2 = Preprocessor(preprocess_train_model_1, preprocess_test_model_1)\n",
    "preprocessor_meta = Preprocessor(preprocess_train_meta_model, preprocess_test_meta_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "least_square_model_1 = LeastSquares(preprocessor_1)\n",
    "least_square_model_2 = LeastSquares(preprocessor_2)\n",
    "\n",
    "least_square_meta_model = LeastSquares(preprocessor_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_ensembler = Model_Ensembler([least_square_model_1, least_square_model_2], least_square_meta_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "least_square_model_1 = LeastSquares(preprocessor_1)\n",
    "least_square_model_1.train(y_train[:half_index], x_train[:half_index])\n",
    "\n",
    "least_square_model_2 = LeastSquares(preprocessor_2)\n",
    "least_square_model_2.train(y_train[:half_index], x_train[:half_index])\n",
    "\n",
    "models = [least_square_model_1, least_square_model_2]\n",
    "\n",
    "stage_0_results = np.hstack([model.predict(x_train[half_index:]) for model in models])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logistic_regression = LogisticRegression(preprocessor_1, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accuracy = cv.cross_validation(y_train, x_train, 5, least_square_model_1, seed=1, compute_loss=imp.rmse)\n",
    "accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sum(accuracy) / len(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
